{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# Homework 2: Language Modeling\n",
        "11-411/611 Natural Language Processing (Fall 2024)\n",
        "\n",
        "- RELEASED: October 1 2024\n",
        "- DUE: October 24 2024 11:59 pm EDT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e485bc65",
      "metadata": {
        "id": "e485bc65"
      },
      "source": [
        "### Submission Guidelines\n",
        "**Programming:** \n",
        "- This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit `ngram_lm.py` and `rnn_lm.py` on Gradescope.\n",
        "- We recommended that you first code in the notebook and then copy the corresponding methods/classes to `ngram_lm.py` and `rnn_lm.py`.\n",
        "\n",
        "**Written:**\n",
        "- Analysis questions would require you to run your code.\n",
        "- You need to write your answers in a document and upload it alongside the programming components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjvLk7XPMxnl",
      "metadata": {
        "id": "wjvLk7XPMxnl"
      },
      "source": [
        "### Upload (if using Colab) main.py and utils.py, and the data.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "id": "bmEQByNzNI4d"
      },
      "outputs": [],
      "source": [
        "#!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models [60 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "#!pip install requests\n",
        "#!pip install torch\n",
        "#!pip install tqdm\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )\n",
        "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ed9e54f",
      "metadata": {
        "id": "9ed9e54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence START symbol: <s>\n",
            "Sentence END symbol: </s>\n",
            "Unknown word symbol: <UNK>\n"
          ]
        }
      ],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n",
        "print(\"Unknown word symbol: {}\".format(UNK))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d60ce7c2",
      "metadata": {
        "id": "d60ce7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n"
          ]
        }
      ],
      "source": [
        "# Read the sample file\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ec373cc",
      "metadata": {
        "id": "4ec373cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>']\n",
            "['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = preprocess(sample, n=3)\n",
        "for s in sample:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "36a2a96e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Flattens a nested list into a 1D list.\n",
        "flattened = flatten(sample)\n",
        "print(flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 1: N-Gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "#### TODO: Defining `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "    \n",
        "    Returns:\n",
        "        n_grams_list: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "    n_grams_list = []\n",
        "    for words in range(len(list_of_words)):\n",
        "        n_grams_list.append(tuple(list_of_words[words:words+n]))\n",
        "    if n==1:\n",
        "        return n_grams_list\n",
        "    return n_grams_list[:-(n-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5fdab35a",
      "metadata": {
        "id": "5fdab35a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: get_ngrams()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=3)\n",
        "flattened = flatten(sample)\n",
        "#print(get_ngrams(flattened, 3))\n",
        "\n",
        "# get_ngrams() return n_grams tuples\n",
        "assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'never'),\n",
        "        ('are', 'never', 'ever'),\n",
        "        ('never', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'getting'),\n",
        "        ('ever', 'getting', 'back'),\n",
        "        ('getting', 'back', 'together'),\n",
        "        ('back', 'together', '</s>'),\n",
        "        ('together', '</s>', '<s>'),\n",
        "        ('</s>', '<s>', '<s>'),\n",
        "        ('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'the'),\n",
        "        ('are', 'the', 'ones'),\n",
        "        ('the', 'ones', 'together'),\n",
        "        ('ones', 'together', 'we'),\n",
        "        ('together', 'we', 'are'),\n",
        "        ('we', 'are', 'back'),\n",
        "        ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "#### **TODO:** Class `NGramLanguageModel()`\n",
        "\n",
        "*Now*, we will define our LanguageModel class.\n",
        "\n",
        "**Some Useful Variables:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities, keys being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts, keys being the words themselves and the values being their frequency.\n",
        "- self.n: `int` value for n-gram order (e.g. 1, 2, 3).\n",
        "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the language model\n",
        "- self.smoothing: `float` flag signifying the smoothing parameter.\n",
        "\n",
        "In `lm.py`, we will be taking most of these argumemts from the command line using this command:\n",
        "\n",
        "`python3 lm.py --train data/sample.txt --test data/sample.txt --n 3 --smoothing 0 --min_freq 1`\n",
        "\n",
        "Note that we will not be using log probabilities in this section. Store the probabilities as they are, not in log space.\n",
        "\n",
        "**Laplace Smoothing**\n",
        "\n",
        "There are two ways to perform this:\n",
        "- Either you calculate all possible n-grams at train time and calculate smooth probabilities for all of them, hence inflating the model (eager emoothing). You then use the probabilities as when required at test time. **OR**\n",
        "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use (lazy smoothing).\n",
        "\n",
        "You will be implementing lazy smoothing\n",
        "\n",
        "**Perplexity**\n",
        "\n",
        "Steps:\n",
        "1. Flatten the test data.\n",
        "2. Extract ngrams from the flattened data.\n",
        "3. Calculate perplexity according to given formula. For unseen n-grams, calculate using smoothed likelihood and store the unseen n-gram probability in the labguage model `model` attribute:\n",
        "\n",
        "$ppl(W_{test}) = ppl(W_1W_2 ... W_n)^{-1/n} $\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes to summation under `log`. Take the log of probabilities, sum them up, and then exponentiate it to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams()`.\n",
        "- The test suite provided is **not exhaustive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#   \n",
        "# Copyright (C) 2024\n",
        "# \n",
        "# @author: Ezra Fu <erzhengf@andrew.cmu.edu>\n",
        "# based on work by \n",
        "# Ishita <igoyal@andrew.cmu.edu> \n",
        "# Suyash <schavan@andrew.cmu.edu>\n",
        "# Abhishek <asrivas4@andrew.cmu.edu>\n",
        "\n",
        "\"\"\"\n",
        "11-411/611 NLP Assignment 2\n",
        "N-gram Language Model Implementation\n",
        "\n",
        "Complete the LanguageModel class and other TO-DO methods.\n",
        "\"\"\"\n",
        "\n",
        "#######################################\n",
        "# Import Statements\n",
        "#######################################\n",
        "from utils import *\n",
        "from collections import Counter\n",
        "from itertools import product\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "\n",
        "#######################################\n",
        "# TODO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "    \n",
        "    Returns:\n",
        "        n_grams_list: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "    n_grams_list = []\n",
        "    for words in range(len(list_of_words)):\n",
        "        n_grams_list.append(tuple(list_of_words[words:words+n]))\n",
        "    if n==1:\n",
        "        return n_grams_list\n",
        "    return n_grams_list[:-(n-1)]\n",
        "\n",
        "#######################################\n",
        "# TODO: NGramLanguageModel()\n",
        "#######################################\n",
        "class NGramLanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        \"\"\"\n",
        "        Language model class.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        n: int\n",
        "            n-gram order\n",
        "        train_data: List[List]\n",
        "            already preprocessed unflattened list of sentences. e.g. [[\"<s>\", \"hello\", \"my\", \"</s>\"], [\"<s>\", \"hi\", \"there\", \"</s>\"]]\n",
        "        alpha: float\n",
        "            Smoothing parameter\n",
        "        \n",
        "        Other attributes:\n",
        "            self.tokens: list of individual tokens present in the training corpus\n",
        "            self.vocab: vocabulary dict with counts\n",
        "            self.model: n-gram language model, i.e., n-gram dict with probabilties\n",
        "            self.n_grams_counts: dictionary for storing the frequency of ngrams in the training data, keys being the tuple of words(n-grams) and value being their frequency\n",
        "            self.prefix_counts: dictionary for storing the frequency of the (n-1) grams in the data, similar to the self.n_grams_counts\n",
        "            As an example:\n",
        "            For a trigram model, the n-gram would be (w1,w2,w3), the corresponding [n-1] gram would be (w1,w2)\n",
        "        \"\"\"\n",
        "        self.n = n \n",
        "        self.train_data = train_data\n",
        "        self.alpha = alpha\n",
        "        self.tokens = flatten(self.train_data)\n",
        "        self.vocab = Counter(self.tokens)\n",
        "        # Initialize the n_grams_counts and prefix_counts before setting up the model\n",
        "        self.n_grams_counts = {}\n",
        "        self.prefix_counts = {}\n",
        "        for ngram in get_ngrams(self.tokens, self.n):\n",
        "            self.n_grams_counts[ngram] = self.n_grams_counts.get(ngram,0) + 1\n",
        "            if self.n > 1:\n",
        "                prefix = ngram[:-1]\n",
        "                self.prefix_counts[prefix] = self.prefix_counts.get(prefix,0) + 1\n",
        "        \n",
        "        # Now initialize the model using the n_grams_counts\n",
        "        self.model = {}\n",
        "        for ngram in get_ngrams(self.tokens, self.n):\n",
        "            self.model[ngram] = self.get_prob(ngram)\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Returns a n-gram dict with their smoothed probabilities. Remember to consider the edge case of n=1 as well\n",
        "        \n",
        "        You are expected to update the self.n_grams_counts and self.prefix_counts, and use those calculate the probabilities. \n",
        "        \"\"\"\n",
        "        if not self.tokens:\n",
        "            print(\"Warning: No tokens found in training data.\")\n",
        "            return {}\n",
        "\n",
        "        for ngram in get_ngrams(self.tokens, self.n):\n",
        "            self.model[ngram] = self.get_prob(ngram)\n",
        "        return self.model\n",
        "            \n",
        "        #for i in range(len(self.tokens) - self.n + 1):\n",
        "        #    ngram = tuple(self.tokens[i:i+self.n])\n",
        "        #    #print(f\"Processing ngram: {ngram}\")\n",
        "        #    self.n_grams_counts[ngram] = self.n_grams_counts.get(ngram,0) + 1\n",
        "#\n",
        "        #    if self.n > 1:\n",
        "        #        prefix = tuple(self.tokens[i:i+self.n-1])\n",
        "        #        self.prefix_counts[prefix] = self.prefix_counts.get(prefix,0) + 1\n",
        "        #\n",
        "        #for ngram, count in self.n_grams_counts.items():\n",
        "        #    if self.n == 1:\n",
        "        #        self.model[ngram] = (count + self.alpha) / (len(self.tokens) + self.alpha * len(self.vocab))\n",
        "        #    else:\n",
        "        #        prefix = ngram[:-1]\n",
        "        #        self.model[ngram] = (count + self.alpha) / (self.prefix_counts[prefix] + self.alpha * len(self.vocab))\n",
        "    #\n",
        "        #return self.model\n",
        "\n",
        "    def get_smooth_probabilities(self, ngrams):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of the n-gram, using Laplace Smoothing. \n",
        "        Remember to consider the edge case of  n = 1\n",
        "        HINT: Use self.n_gram_counts, self.tokens and self.prefix_counts \n",
        "        \"\"\"\n",
        "        #print(ngrams)\n",
        "        ngrams_counts = self.n_grams_counts.get(ngrams,0)\n",
        "        #print(ngrams_counts)\n",
        "        if self.n > 1:\n",
        "            prefix_counts = self.prefix_counts.get(ngrams[:-1],0)\n",
        "            #print(f\"Prefix Counts: {prefix_counts}\")\n",
        "            smooth_probabilities = (ngrams_counts + self.alpha) / (prefix_counts + self.alpha * len(self.vocab))\n",
        "        elif self.n == 1:\n",
        "            smooth_probabilities = (ngrams_counts + self.alpha) / (len(self.tokens) + self.alpha * len(self.vocab))\n",
        "        return smooth_probabilities\n",
        "\n",
        "    def get_prob(self, ngram):\n",
        "        \"\"\"\n",
        "        Returns the probability of the n-gram, using Laplace Smoothing.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        ngram: tuple\n",
        "            n-gram tuple\n",
        "        \n",
        "        Returns\n",
        "        _______\n",
        "        float\n",
        "            probability of the n-gram\n",
        "        \"\"\"\n",
        "        probability = self.model.get(ngram,0)\n",
        "        if probability == 0:\n",
        "            # If the n-gram is not observed in the training data, use smoothed probabilities\n",
        "            probability = self.get_smooth_probabilities(ngram)\n",
        "\n",
        "        return probability\n",
        "\n",
        "\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        \"\"\"\n",
        "        Returns perplexity calculated on the test data.\n",
        "        Args\n",
        "        ----------\n",
        "        test_data: List[List] \n",
        "            Already preprocessed nested list of sentences\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Calculated perplexity value\n",
        "        \"\"\"\n",
        "        # Flatten the test data\n",
        "        flattened_test_data = flatten(test_data)\n",
        "        test_ngrams = get_ngrams(flattened_test_data, self.n)\n",
        "        N = len(test_ngrams)\n",
        "        log_prob_sum = 0\n",
        "\n",
        "        for ngrams in test_ngrams:\n",
        "            prob = self.get_prob(ngrams)\n",
        "            log_prob_sum += math.log(prob)\n",
        "\n",
        "        perplexity = math.exp(-log_prob_sum/N)\n",
        "        # Return perplexity using exponentiation of the negative average log probability\n",
        "        return perplexity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: NGramLanguageModel()\n",
        "#######################################\n",
        "# For the sake of understanding we will pass alpha as 0 (no smoothing), so that you gain intuition about the probabilities\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "test_lm.build()\n",
        "test_lm.vocab\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model =={('<s>', 'we'): 1.0,\n",
        "        ('we', 'are'): 1.0,\n",
        "        ('are', 'never'): 0.3333333333333333,\n",
        "        ('never', 'ever'): 1.0,\n",
        "        ('ever', 'ever'): 0.75,\n",
        "        ('ever', 'getting'): 0.25,\n",
        "        ('getting', 'back'): 1.0,\n",
        "        ('back', 'together'): 0.5,\n",
        "        ('together', '</s>'): 0.5,\n",
        "        ('</s>', '<s>'): 1.0,\n",
        "        ('are', 'the'): 0.3333333333333333,\n",
        "        ('the', 'ones'): 1.0,\n",
        "        ('ones', 'together'): 1.0,\n",
        "        ('together', 'we'): 0.5,\n",
        "        ('are', 'back'): 0.3333333333333333,\n",
        "        ('back', '</s>'): 0.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{('<s>', 'we'): 0.23076923076923078, ('we', 'are'): 0.2857142857142857, ('are', 'never'): 0.14285714285714285, ('never', 'ever'): 0.16666666666666666, ('ever', 'ever'): 0.26666666666666666, ('ever', 'getting'): 0.13333333333333333, ('getting', 'back'): 0.16666666666666666, ('back', 'together'): 0.15384615384615385, ('together', '</s>'): 0.15384615384615385, ('</s>', '<s>'): 0.16666666666666666, ('are', 'the'): 0.14285714285714285, ('the', 'ones'): 0.16666666666666666, ('ones', 'together'): 0.16666666666666666, ('together', 'we'): 0.15384615384615385, ('are', 'back'): 0.14285714285714285, ('back', '</s>'): 0.15384615384615385}\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST smoothing: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "print(test_lm.build())\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model =={('<s>', 'we'): 0.23076923076923078,\n",
        "        ('we', 'are'): 0.2857142857142857,\n",
        "        ('are', 'never'): 0.14285714285714285,\n",
        "        ('never', 'ever'): 0.16666666666666666,\n",
        "        ('ever', 'ever'): 0.26666666666666666,\n",
        "        ('ever', 'getting'): 0.13333333333333333,\n",
        "        ('getting', 'back'): 0.16666666666666666,\n",
        "        ('back', 'together'): 0.15384615384615385,\n",
        "        ('together', '</s>'): 0.15384615384615385,\n",
        "        ('</s>', '<s>'): 0.16666666666666666,\n",
        "        ('are', 'the'): 0.14285714285714285,\n",
        "        ('the', 'ones'): 0.16666666666666666,\n",
        "        ('ones', 'together'): 0.16666666666666666,\n",
        "        ('together', 'we'): 0.15384615384615385,\n",
        "        ('are', 'back'): 0.14285714285714285,\n",
        "        ('back', '</s>'): 0.15384615384615385}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST unigram: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=1)\n",
        "test_lm = NGramLanguageModel(n=1, train_data=sample, alpha=1)\n",
        "test_lm.build()\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model == {('<s>',): 0.09090909090909091,\n",
        "        ('we',): 0.12121212121212122,\n",
        "        ('are',): 0.12121212121212122,\n",
        "        ('never',): 0.06060606060606061,\n",
        "        ('ever',): 0.15151515151515152,\n",
        "        ('getting',): 0.06060606060606061,\n",
        "        ('back',): 0.09090909090909091,\n",
        "        ('together',): 0.09090909090909091,\n",
        "        ('</s>',): 0.09090909090909091,\n",
        "        ('the',): 0.06060606060606061,\n",
        "        ('ones',): 0.06060606060606061}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2927e9aa",
      "metadata": {
        "id": "2927e9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4859942891369486\n",
            "5.283124177782943\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_lm\u001b[38;5;241m.\u001b[39mperplexity(sample))\n\u001b[1;32m     19\u001b[0m test_ppl \u001b[38;5;241m=\u001b[39m test_lm\u001b[38;5;241m.\u001b[39mperplexity(sample)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: perplexity()\n",
        "#######################################\n",
        "\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "test_lm.build()\n",
        "test_lm.get_smooth_probabilities(('<s>', 'we'))\n",
        "print(test_lm.perplexity(sample))\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "\n",
        "assert test_ppl < 1.7\n",
        "assert test_ppl > 0\n",
        "\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_lm.build()\n",
        "test_lm.get_smooth_probabilities(('<s>', 'we'))\n",
        "print(test_lm.perplexity(sample))\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "\n",
        "assert test_ppl < 5.0\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fda0b2",
      "metadata": {},
      "source": [
        "### Step 2: RNN Language Model\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. Unlike traditional neural networks, which assume independence among inputs, RNNs utilize their internal state (memory) to process sequences of inputs. This makes them particularly well-suited for tasks where context and order matter.\n",
        "\n",
        "Before diving into building RNN Language Models using PyTorch, it's essential to have a solid foundation in the following areas:\n",
        ". We assume you have had a basic understanding of PyTorch and its core concepts, including tensors, autograd, modules (nn.Module), and how to construct simple neural networks using PyTorch. For more comprehensive learning, refer to the [PyTorch official tutorials](https://pytorch.org/tutorials/) and documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454617c7",
      "metadata": {},
      "source": [
        "#### Preparing the Data\n",
        "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks. You can download the 50d embeddings for this assignment from [Canvas](https://canvas.cmu.edu/courses/39596/files/10855662?module_item_id=5748476).\n",
        "\n",
        "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
        "\n",
        "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
        "\n",
        "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a4411999",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9e9b6d54",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Path to the GloVe file\n",
        "glove_path = 'data/glove.6B.50d.txt'  # Update this path\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "#print(f\"glove_embeddings:{glove_embeddings}\")\n",
        "\n",
        "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in embeddings_dict:\n",
        "            embedding_matrix[ix] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "#print(f\"embedding_matrix:{embedding_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbade19",
      "metadata": {},
      "source": [
        "#### TODO: Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bc88721e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#   \n",
        "# Copyright (C) 2024\n",
        "# \n",
        "# @author: Ezra Fu <erzhengf@andrew.cmu.edu>\n",
        "# based on work by \n",
        "# Ishita <igoyal@andrew.cmu.edu> \n",
        "# Suyash <schavan@andrew.cmu.edu>\n",
        "# Abhishek <asrivas4@andrew.cmu.edu>\n",
        "\n",
        "\"\"\"\n",
        "11-411/611 NLP Assignment 2\n",
        "RNN Language Model Implementation\n",
        "\n",
        "Complete the LanguageModel class and other TO-DO methods.\n",
        "\"\"\"\n",
        "\n",
        "#######################################\n",
        "# Import Statements\n",
        "#######################################\n",
        "from utils import *\n",
        "from collections import Counter\n",
        "from itertools import product\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#######################################\n",
        "# TODO: RNNLanguageModel()\n",
        "#######################################\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        \"\"\"\n",
        "        RNN model class.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        vocab_size: int\n",
        "            Size of the vocabulary\n",
        "        embedding_dim: int\n",
        "            Dimension of the word embeddings\n",
        "        hidden_dim: int\n",
        "            Dimension of the hidden state of the RNN\n",
        "        embedding_matrix: torch.Tensor\n",
        "            Pre-trained GloVe embeddings\n",
        "            \n",
        "        Other attributes:\n",
        "            self.embedding: nn.Embedding\n",
        "                Embedding layer\n",
        "            self.rnn: nn.RNN\n",
        "                RNN layer\n",
        "            self.fc: nn.Linear\n",
        "                Fully connected layer\n",
        "        \n",
        "        Note: Remember to initialize the weights of the embedding layer with the GloVe embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        The forward pass of the RNN model.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        x: torch.Tensor\n",
        "            Input tensor of shape (batch_size, sequence_length)\n",
        "        hidden: torch.Tensor\n",
        "            Hidden state tensor of shape (num_layers, batch_size, hidden_dim)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out: torch.Tensor\n",
        "            Output tensor of shape (batch_size, sequence_length, vocab_size)\n",
        "        hidden: torch.Tensor\n",
        "            Hidden state tensor of shape (num_layers, batch_size, hidden_dim)\n",
        "            \n",
        "        HINT: You need to use the embedding layer, rnn layer and the fully connected layer to define the forward pass\n",
        "        \"\"\"\n",
        "        embedding_layer = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedding_layer, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    \n",
        "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
        "        \"\"\"\n",
        "        Predicts the next words given a sequence.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        sequence: str\n",
        "            Input sequence\n",
        "        word_to_ix: dict\n",
        "            Dictionary mapping words to their corresponding indices\n",
        "        ix_to_word: dict\n",
        "            Dictionary mapping indices to their corresponding words\n",
        "        num_words: int\n",
        "            Maximum number of words to predict\n",
        "        mode: str\n",
        "            Mode of prediction. 'max' or 'multinomial'\n",
        "            'max' mode selects the word with maximum probability\n",
        "            'multinomial' mode samples the word from the probability distribution\n",
        "            Hint: Use torch.multinomial() method\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        predicted_sequence: List[str]\n",
        "            List of predicted words\n",
        "        \"\"\"\n",
        "        self.eval()  # Set the model to evaluation mode\n",
        "        sequence = sequence.split()  # Convert string to list of words\n",
        "        predicted_sequence = []\n",
        "        with torch.no_grad():\n",
        "            input_seq = [word_to_ix.get(word, word_to_ix['<UNK>']) for word in sequence]\n",
        "            input = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)  # Shape: (1, sequence_length)\n",
        "            #print(f\"Input sequence: {input}\")\n",
        "            #print(f\"Input Sequence shape: {input.shape}\")\n",
        "            hidden = None  # Initialize hidden state\n",
        "\n",
        "            for _ in range(num_words):\n",
        "                output, hidden = self.forward(input, hidden)\n",
        "                #print(f\"Output sequence: {output}\")\n",
        "                #print(f\"Output Sequence shape: {output.shape}\")\n",
        "\n",
        "                # Get the output of the last time step\n",
        "                output = output[:, -1, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "                if mode == 'max':\n",
        "                    predicted_word_ix = torch.argmax(output, dim=1).item()\n",
        "                elif mode == 'multinomial':\n",
        "                    probabilities = torch.softmax(output, dim=1)\n",
        "                    predicted_word_ix = torch.multinomial(probabilities, num_samples=1).item()\n",
        "                else:\n",
        "                    raise ValueError(\"Invalid mode. Choose 'max' or 'multinomial'.\")\n",
        "\n",
        "                predicted_word = ix_to_word.get(predicted_word_ix, '<UNK>')\n",
        "                #print(f\"Predicted word: {predicted_word}\")\n",
        "                predicted_sequence.append(predicted_word)\n",
        "\n",
        "                # Prepare input for the next time step\n",
        "                input = torch.tensor([[predicted_word_ix]], dtype=torch.long)\n",
        "        return predicted_sequence\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6c516d",
      "metadata": {},
      "source": [
        "#### Training the Model\n",
        "The following code snippet provided is responsible for training the RNN language model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "96135209",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 2.6288723945617676, Perplexity: 13.858134580632937\n",
            "Epoch 2/10, Loss: 2.4372386932373047, Perplexity: 11.441403857477876\n",
            "Epoch 3/10, Loss: 2.2839365005493164, Perplexity: 9.815242166724511\n",
            "Epoch 4/10, Loss: 2.1605491638183594, Perplexity: 8.675900841293625\n",
            "Epoch 5/10, Loss: 2.0542263984680176, Perplexity: 7.800800826332901\n",
            "Epoch 6/10, Loss: 1.956910252571106, Perplexity: 7.0774257897383395\n",
            "Epoch 7/10, Loss: 1.8642698526382446, Perplexity: 6.451223821896329\n",
            "Epoch 8/10, Loss: 1.7739530801773071, Perplexity: 5.8941072473852\n",
            "Epoch 9/10, Loss: 1.6850324869155884, Perplexity: 5.3926261192933875\n",
            "Epoch 10/10, Loss: 1.5976566076278687, Perplexity: 4.9414391151344885\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: RNNLanguageModel() and training\n",
        "#######################################\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ba8069",
      "metadata": {
        "id": "14ba8069"
      },
      "source": [
        "## Part 2: Written [40 points]. We have given some code for some of the written parts to make it easier for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fcc2e0",
      "metadata": {},
      "source": [
        "### **Written 4.1** – n-gram counts [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1db54901",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19990\n",
            "Business data with bi-gram: 83819\n",
            "9611\n",
            "Sports data with bi-gram: 77398\n",
            "19990\n",
            "Business data with tri-gram: 141221\n",
            "9611\n",
            "Sports data with tri-gram: 135645\n"
          ]
        }
      ],
      "source": [
        "def n_gram_counts(file,n,smoothing):\n",
        "    data = read_file(file)\n",
        "    train = preprocess(data,n)\n",
        "    print(len(train))\n",
        "    lm = NGramLanguageModel(n, train, smoothing)\n",
        "    lm.build()\n",
        "    lm.vocab\n",
        "    return len(lm.n_grams_counts.keys())\n",
        "\n",
        "business_data = 'data/bbc/business.txt'\n",
        "sports_data = 'data/bbc/sport.txt'\n",
        "print(f'Business data with bi-gram: {n_gram_counts(file=business_data,n=2,smoothing=0.1)}')\n",
        "print(f'Sports data with bi-gram: {n_gram_counts(file=sports_data,n=2,smoothing=0.1)}')\n",
        "print(f'Business data with tri-gram: {n_gram_counts(file=business_data,n=3,smoothing=0.1)}')\n",
        "print(f'Sports data with tri-gram: {n_gram_counts(file=sports_data,n=3,smoothing=0.1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53431996",
      "metadata": {
        "id": "53431996"
      },
      "source": [
        "### **Written 4.2** – Song Attribution [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4751ea5b",
      "metadata": {
        "id": "4751ea5b"
      },
      "outputs": [],
      "source": [
        "# Example code for Taylor Swift N-Gram LM\n",
        "def song_attribution(train_data, test_data, n, smoothing):\n",
        "    train = read_file(train_data)\n",
        "    test = read_file(test_data)\n",
        "    train = preprocess(train, n)\n",
        "    test = preprocess(test, n)\n",
        "    lm = NGramLanguageModel(n, train, smoothing)\n",
        "    lm.build()\n",
        "\n",
        "    ppl = lm.perplexity(test)\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6e5a5958",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taylor Swift PPL with tri-gram: 138.00663307990817\n"
          ]
        }
      ],
      "source": [
        "train_data=\"data/lyrics/taylor_swift.txt\"\n",
        "test_data = \"data/lyrics/test_lyrics.txt\"\n",
        "print(f'Taylor Swift PPL with tri-gram: {song_attribution(train_data, test_data, 3, 0.1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "20f24f74",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Green Day PPL with tri-gram: 522.5401188730924\n"
          ]
        }
      ],
      "source": [
        "train_data=\"data/lyrics/green_day.txt\"\n",
        "test_data = \"data/lyrics/test_lyrics.txt\"\n",
        "print(f'Green Day PPL with tri-gram: {song_attribution(train_data, test_data, 3, 0.1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c0146742",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ed Sheeran PPL with tri-gram: 521.2574891234094\n"
          ]
        }
      ],
      "source": [
        "train_data=\"data/lyrics/ed_sheeran.txt\"\n",
        "test_data = \"data/lyrics/test_lyrics.txt\"\n",
        "print(f'Ed Sheeran PPL with tri-gram: {song_attribution(train_data, test_data, 3, 0.1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749a49a0",
      "metadata": {
        "id": "749a49a0"
      },
      "source": [
        "### **Written 4.3.1** –  Intro to Decoding [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6949f6a5",
      "metadata": {},
      "source": [
        "Please take a look at and understand the functions: `best_candidate()`, `top_k_best_candidates()` and `generate_sentences_from_phrase()` in `utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c8ed9b85",
      "metadata": {
        "id": "c8ed9b85"
      },
      "outputs": [],
      "source": [
        "n = 4\n",
        "smoothing = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "96b3d000",
      "metadata": {
        "id": "96b3d000"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('<s>', '<s>', '<s>', 'one'): 0.00322040072859745,\n",
              " ('<s>', '<s>', 'one', 'two'): 0.002781289506953224,\n",
              " ('<s>', 'one', 'two', 'three'): 0.002937249666221629,\n",
              " ('one', 'two', 'three', 'four'): 0.002937249666221629,\n",
              " ('two', 'three', 'four', '</s>'): 0.002937249666221629,\n",
              " ('three', 'four', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('four', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('</s>', '<s>', '<s>', '<s>'): 0.9455804124462581,\n",
              " ('<s>', '<s>', '<s>', 'ooh'): 0.0051147540983606556,\n",
              " ('<s>', '<s>', 'ooh', 'ooh'): 0.03696450428396573,\n",
              " ('<s>', 'ooh', 'ooh', '</s>'): 0.00797940797940798,\n",
              " ('ooh', 'ooh', '</s>', '<s>'): 0.03138780804150454,\n",
              " ('ooh', '</s>', '<s>', '<s>'): 0.06533166458072591,\n",
              " ('<s>', '<s>', '<s>', 'every'): 0.003074681238615665,\n",
              " ('<s>', '<s>', 'every', 'time'): 0.010392902408111533,\n",
              " ('<s>', 'every', 'time', 'you'): 0.0029139072847682124,\n",
              " ('every', 'time', 'you', 'come'): 0.002937249666221629,\n",
              " ('time', 'you', 'come', 'around'): 0.002937249666221629,\n",
              " ('you', 'come', 'around', 'you'): 0.002937249666221629,\n",
              " ('come', 'around', 'you', 'know'): 0.002937249666221629,\n",
              " ('around', 'you', 'know', 'i'): 0.002937249666221629,\n",
              " ('you', 'know', 'i', 'can'): 0.007779171894604768,\n",
              " ('know', 'i', 'can', 't'): 0.01086092715231788,\n",
              " ('i', 'can', 't', 'say'): 0.002699386503067485,\n",
              " ('can', 't', 'say', 'no'): 0.002937249666221629,\n",
              " ('t', 'say', 'no', '</s>'): 0.002937249666221629,\n",
              " ('say', 'no', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('no', '</s>', '<s>', '<s>'): 0.06062893081761007,\n",
              " ('<s>', 'every', 'time', 'the'): 0.0029139072847682124,\n",
              " ('every', 'time', 'the', 'sun'): 0.002937249666221629,\n",
              " ('time', 'the', 'sun', 'goes'): 0.002937249666221629,\n",
              " ('the', 'sun', 'goes', 'down'): 0.01086092715231788,\n",
              " ('sun', 'goes', 'down', 'i'): 0.0029139072847682124,\n",
              " ('goes', 'down', 'i', 'let'): 0.002937249666221629,\n",
              " ('down', 'i', 'let', 'you'): 0.002937249666221629,\n",
              " ('i', 'let', 'you', 'take'): 0.002937249666221629,\n",
              " ('let', 'you', 'take', 'control'): 0.002937249666221629,\n",
              " ('you', 'take', 'control', '</s>'): 0.002937249666221629,\n",
              " ('take', 'control', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('control', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'i'): 0.1257704918032787,\n",
              " ('<s>', '<s>', 'i', 'can'): 0.02191670036393045,\n",
              " ('<s>', 'i', 'can', 'feel'): 0.002746566791510612,\n",
              " ('i', 'can', 'feel', 'the'): 0.0029216467463479417,\n",
              " ('can', 'feel', 'the', 'paradise'): 0.002937249666221629,\n",
              " ('feel', 'the', 'paradise', 'before'): 0.002937249666221629,\n",
              " ('the', 'paradise', 'before', 'my'): 0.002937249666221629,\n",
              " ('paradise', 'before', 'my', 'world'): 0.002937249666221629,\n",
              " ('before', 'my', 'world', 'implodes'): 0.002937249666221629,\n",
              " ('my', 'world', 'implodes', '</s>'): 0.002937249666221629,\n",
              " ('world', 'implodes', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('implodes', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'and'): 0.105224043715847,\n",
              " ('<s>', '<s>', 'and', 'tonight'): 0.0010041077133728891,\n",
              " ('<s>', 'and', 'tonight', 'i'): 0.002937249666221629,\n",
              " ('and', 'tonight', 'i', 'had'): 0.002937249666221629,\n",
              " ('tonight', 'i', 'had', 'something'): 0.002937249666221629,\n",
              " ('i', 'had', 'something', 'wonderful'): 0.002937249666221629,\n",
              " ('had', 'something', 'wonderful', '</s>'): 0.002937249666221629,\n",
              " ('something', 'wonderful', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('wonderful', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'my'): 0.013420765027322404,\n",
              " ('<s>', '<s>', 'my', 'bad'): 0.03029001074113856,\n",
              " ('<s>', 'my', 'bad', 'habits'): 0.03638709677419355,\n",
              " ('my', 'bad', 'habits', 'lead'): 0.03380645161290322,\n",
              " ('bad', 'habits', 'lead', 'to'): 0.03389391979301423,\n",
              " ('habits', 'lead', 'to', 'late'): 0.008020698576972834,\n",
              " ('lead', 'to', 'late', 'nights'): 0.008233731739707836,\n",
              " ('to', 'late', 'nights', 'endin'): 0.008233731739707836,\n",
              " ('late', 'nights', 'endin', 'alone'): 0.008233731739707836,\n",
              " ('nights', 'endin', 'alone', '</s>'): 0.008233731739707836,\n",
              " ('endin', 'alone', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('alone', '</s>', '<s>', '<s>'): 0.06533166458072591,\n",
              " ('<s>', '<s>', '<s>', 'conversations'): 0.0004517304189435337,\n",
              " ('<s>', '<s>', 'conversations', 'with'): 0.008233731739707836,\n",
              " ('<s>', 'conversations', 'with', 'a'): 0.008233731739707836,\n",
              " ('conversations', 'with', 'a', 'stranger'): 0.008233731739707836,\n",
              " ('with', 'a', 'stranger', 'i'): 0.008233731739707836,\n",
              " ('a', 'stranger', 'i', 'barely'): 0.008233731739707836,\n",
              " ('stranger', 'i', 'barely', 'know'): 0.008233731739707836,\n",
              " ('i', 'barely', 'know', '</s>'): 0.008233731739707836,\n",
              " ('barely', 'know', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('know', '</s>', '<s>', '<s>'): 0.14062140391254316,\n",
              " ('<s>', '<s>', '<s>', 'swearin'): 0.0004517304189435337,\n",
              " ('<s>', '<s>', 'swearin', 'this'): 0.008233731739707836,\n",
              " ('<s>', 'swearin', 'this', 'will'): 0.008233731739707836,\n",
              " ('swearin', 'this', 'will', 'be'): 0.008233731739707836,\n",
              " ('this', 'will', 'be', 'the'): 0.008233731739707836,\n",
              " ('will', 'be', 'the', 'last'): 0.008233731739707836,\n",
              " ('be', 'the', 'last', 'but'): 0.008168642951251647,\n",
              " ('the', 'last', 'but', 'it'): 0.008233731739707836,\n",
              " ('last', 'but', 'it', 'probably'): 0.008233731739707836,\n",
              " ('but', 'it', 'probably', 'won'): 0.008233731739707836,\n",
              " ('it', 'probably', 'won', 't'): 0.008233731739707836,\n",
              " ('probably', 'won', 't', '</s>'): 0.008233731739707836,\n",
              " ('won', 't', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('t', '</s>', '<s>', '<s>'): 0.021231979030144166,\n",
              " ('<s>', '<s>', 'i', 'got'): 0.013020622725434696,\n",
              " ('<s>', 'i', 'got', 'nothin'): 0.007958921694480103,\n",
              " ('i', 'got', 'nothin', 'left'): 0.008233731739707836,\n",
              " ('got', 'nothin', 'left', 'to'): 0.008211920529801325,\n",
              " ('nothin', 'left', 'to', 'lose'): 0.008233731739707836,\n",
              " ('left', 'to', 'lose', 'or'): 0.008233731739707836,\n",
              " ('to', 'lose', 'or', 'use'): 0.008233731739707836,\n",
              " ('lose', 'or', 'use', 'or'): 0.008233731739707836,\n",
              " ('or', 'use', 'or', 'do'): 0.008233731739707836,\n",
              " ('use', 'or', 'do', '</s>'): 0.008233731739707836,\n",
              " ('or', 'do', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('do', '</s>', '<s>', '<s>'): 0.10348139255702281,\n",
              " ('habits', 'lead', 'to', 'wide'): 0.008020698576972834,\n",
              " ('lead', 'to', 'wide', 'eyes'): 0.008233731739707836,\n",
              " ('to', 'wide', 'eyes', 'stare'): 0.008233731739707836,\n",
              " ('wide', 'eyes', 'stare', 'into'): 0.008233731739707836,\n",
              " ('eyes', 'stare', 'into', 'space'): 0.008233731739707836,\n",
              " ('stare', 'into', 'space', '</s>'): 0.008233731739707836,\n",
              " ('into', 'space', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('space', '</s>', '<s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', 'and', 'i'): 0.1881332724783204,\n",
              " ('<s>', 'and', 'i', 'know'): 0.03123382226056946,\n",
              " ('and', 'i', 'know', 'i'): 0.01554140127388535,\n",
              " ('i', 'know', 'i', 'll'): 0.010580645161290321,\n",
              " ('know', 'i', 'll', 'lose'): 0.008168642951251647,\n",
              " ('i', 'll', 'lose', 'control'): 0.008233731739707836,\n",
              " ('ll', 'lose', 'control', 'of'): 0.008233731739707836,\n",
              " ('lose', 'control', 'of', 'the'): 0.008233731739707836,\n",
              " ('control', 'of', 'the', 'things'): 0.008233731739707836,\n",
              " ('of', 'the', 'things', 'that'): 0.008233731739707836,\n",
              " ('the', 'things', 'that', 'i'): 0.01080368906455863,\n",
              " ('things', 'that', 'i', 'say'): 0.008190224570673713,\n",
              " ('that', 'i', 'say', '</s>'): 0.008233731739707836,\n",
              " ('i', 'say', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('say', '</s>', '<s>', '<s>'): 0.06533166458072591,\n",
              " ('<s>', '<s>', '<s>', 'yeah'): 0.005989071038251366,\n",
              " ('<s>', '<s>', 'yeah', 'i'): 0.019541616405307598,\n",
              " ('<s>', 'yeah', 'i', 'was'): 0.01074705111402359,\n",
              " ('yeah', 'i', 'was', 'lookin'): 0.008211920529801325,\n",
              " ('i', 'was', 'lookin', 'for'): 0.008233731739707836,\n",
              " ('was', 'lookin', 'for', 'a'): 0.008233731739707836,\n",
              " ('lookin', 'for', 'a', 'way'): 0.008211920529801325,\n",
              " ('for', 'a', 'way', 'out'): 0.008211920529801325,\n",
              " ('a', 'way', 'out', 'now'): 0.008233731739707836,\n",
              " ('way', 'out', 'now', 'i'): 0.008233731739707836,\n",
              " ('out', 'now', 'i', 'can'): 0.008233731739707836,\n",
              " ('now', 'i', 'can', 't'): 0.008233731739707836,\n",
              " ('i', 'can', 't', 'escape'): 0.007607361963190184,\n",
              " ('can', 't', 'escape', '</s>'): 0.008233731739707836,\n",
              " ('t', 'escape', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('escape', '</s>', '<s>', '<s>'): 0.01086092715231788,\n",
              " ('<s>', '<s>', '<s>', 'nothin'): 0.0004517304189435337,\n",
              " ('<s>', '<s>', 'nothin', 'happens'): 0.008233731739707836,\n",
              " ('<s>', 'nothin', 'happens', 'aftеr'): 0.0029216467463479417,\n",
              " ('nothin', 'happens', 'aftеr', 'two'): 0.002937249666221629,\n",
              " ('happens', 'aftеr', 'two', 'it'): 0.002937249666221629,\n",
              " ('aftеr', 'two', 'it', 's'): 0.002937249666221629,\n",
              " ('two', 'it', 's', 'true'): 0.008233731739707836,\n",
              " ('it', 's', 'true', 'it'): 0.008020698576972834,\n",
              " ('s', 'true', 'it', 's'): 0.008233731739707836,\n",
              " ('true', 'it', 's', 'true'): 0.008233731739707836,\n",
              " ('it', 's', 'true', '</s>'): 0.02613195342820181,\n",
              " ('s', 'true', '</s>', '<s>'): 0.02633637548891786,\n",
              " ('true', '</s>', '<s>', '<s>'): 0.03138780804150454,\n",
              " ('habits', 'lead', 'to', 'you'): 0.018369987063389392,\n",
              " ('lead', 'to', 'you', '</s>'): 0.018659658344283837,\n",
              " ('to', 'you', '</s>', '<s>'): 0.06533166458072591,\n",
              " ('you', '</s>', '<s>', '<s>'): 0.3853497942386831,\n",
              " ('<s>', 'ooh', 'ooh', 'ooh'): 0.031145431145431144,\n",
              " ('ooh', 'ooh', 'ooh', 'ooh'): 0.05885225885225886,\n",
              " ('ooh', 'ooh', 'ooh', '</s>'): 0.022222222222222223,\n",
              " ('my', 'bad', 'habits', 'lеad'): 0.002838709677419355,\n",
              " ('bad', 'habits', 'lеad', 'to'): 0.002937249666221629,\n",
              " ('habits', 'lеad', 'to', 'you'): 0.002937249666221629,\n",
              " ('lеad', 'to', 'you', '</s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', 'every', 'pure'): 0.002788339670468948,\n",
              " ('<s>', 'every', 'pure', 'intention'): 0.002937249666221629,\n",
              " ('every', 'pure', 'intention', 'ends'): 0.002937249666221629,\n",
              " ('pure', 'intention', 'ends', 'when'): 0.002937249666221629,\n",
              " ('intention', 'ends', 'when', 'the'): 0.002937249666221629,\n",
              " ('ends', 'when', 'the', 'good'): 0.002937249666221629,\n",
              " ('when', 'the', 'good', 'times'): 0.002937249666221629,\n",
              " ('the', 'good', 'times', 'start'): 0.0029139072847682124,\n",
              " ('good', 'times', 'start', '</s>'): 0.002937249666221629,\n",
              " ('times', 'start', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('start', '</s>', '<s>', '<s>'): 0.01086092715231788,\n",
              " ('<s>', '<s>', '<s>', 'fallin'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'fallin', 'over'): 0.002937249666221629,\n",
              " ('<s>', 'fallin', 'over', 'everything'): 0.002937249666221629,\n",
              " ('fallin', 'over', 'everything', 'to'): 0.002937249666221629,\n",
              " ('over', 'everything', 'to', 'reach'): 0.002937249666221629,\n",
              " ('everything', 'to', 'reach', 'the'): 0.002937249666221629,\n",
              " ('to', 'reach', 'the', 'first'): 0.002937249666221629,\n",
              " ('reach', 'the', 'first', 'time'): 0.002937249666221629,\n",
              " ('the', 'first', 'time', 's'): 0.0029294274300932094,\n",
              " ('first', 'time', 's', 'spark'): 0.002937249666221629,\n",
              " ('time', 's', 'spark', '</s>'): 0.002937249666221629,\n",
              " ('s', 'spark', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('spark', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'it'): 0.018666666666666665,\n",
              " ('<s>', '<s>', 'it', 'started'): 0.0021934197407776673,\n",
              " ('<s>', 'it', 'started', 'under'): 0.002937249666221629,\n",
              " ('it', 'started', 'under', 'neon'): 0.002937249666221629,\n",
              " ('started', 'under', 'neon', 'lights'): 0.002937249666221629,\n",
              " ('under', 'neon', 'lights', 'and'): 0.002937249666221629,\n",
              " ('neon', 'lights', 'and', 'then'): 0.002937249666221629,\n",
              " ('lights', 'and', 'then', 'it'): 0.002937249666221629,\n",
              " ('and', 'then', 'it', 'all'): 0.0029294274300932094,\n",
              " ('then', 'it', 'all', 'got'): 0.002937249666221629,\n",
              " ('it', 'all', 'got', 'dark'): 0.002937249666221629,\n",
              " ('all', 'got', 'dark', '</s>'): 0.002937249666221629,\n",
              " ('got', 'dark', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('dark', '</s>', '<s>', '<s>'): 0.021231979030144166,\n",
              " ('<s>', '<s>', 'i', 'only'): 0.0008896077638495754,\n",
              " ('<s>', 'i', 'only', 'know'): 0.002937249666221629,\n",
              " ('i', 'only', 'know', 'how'): 0.002937249666221629,\n",
              " ('only', 'know', 'how', 'to'): 0.002937249666221629,\n",
              " ('know', 'how', 'to', 'go'): 0.002937249666221629,\n",
              " ('how', 'to', 'go', 'too'): 0.002937249666221629,\n",
              " ('to', 'go', 'too', 'far'): 0.002937249666221629,\n",
              " ('go', 'too', 'far', '</s>'): 0.002937249666221629,\n",
              " ('too', 'far', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('far', '</s>', '<s>', '<s>'): 0.008233731739707836,\n",
              " ('<s>', 'nothin', 'happens', 'after'): 0.005577689243027889,\n",
              " ('nothin', 'happens', 'after', 'two'): 0.00559254327563249,\n",
              " ('happens', 'after', 'two', 'it'): 0.00559254327563249,\n",
              " ('after', 'two', 'it', 's'): 0.00559254327563249,\n",
              " ('<s>', '<s>', '<s>', 'we'): 0.022163934426229506,\n",
              " ('<s>', '<s>', 'we', 'took'): 0.002093244529019981,\n",
              " ('<s>', 'we', 'took', 'the'): 0.002937249666221629,\n",
              " ('we', 'took', 'the', 'long'): 0.002937249666221629,\n",
              " ('took', 'the', 'long', 'way'): 0.002937249666221629,\n",
              " ('the', 'long', 'way', 'round'): 0.002937249666221629,\n",
              " ('long', 'way', 'round', '</s>'): 0.002937249666221629,\n",
              " ('way', 'round', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('round', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', 'and', 'burned'): 0.0010041077133728891,\n",
              " ('<s>', 'and', 'burned', 'til'): 0.002937249666221629,\n",
              " ('and', 'burned', 'til', 'the'): 0.002937249666221629,\n",
              " ('burned', 'til', 'the', 'fun'): 0.002937249666221629,\n",
              " ('til', 'the', 'fun', 'ran'): 0.002937249666221629,\n",
              " ('the', 'fun', 'ran', 'out'): 0.002937249666221629,\n",
              " ('fun', 'ran', 'out', 'now'): 0.002937249666221629,\n",
              " ('ran', 'out', 'now', '</s>'): 0.002937249666221629,\n",
              " ('out', 'now', '</s>', '<s>'): 0.01086092715231788,\n",
              " ('now', '</s>', '<s>', '<s>'): 0.22289281997918833,\n",
              " ('<s>', '<s>', 'i', 'have'): 0.008976951071572987,\n",
              " ('<s>', 'i', 'have', 'grown'): 0.0028608582574772435,\n",
              " ('i', 'have', 'grown', 'up'): 0.002937249666221629,\n",
              " ('have', 'grown', 'up', 'i'): 0.0029294274300932094,\n",
              " ('grown', 'up', 'i', 'am'): 0.002937249666221629,\n",
              " ('up', 'i', 'am', 'a'): 0.002937249666221629,\n",
              " ('i', 'am', 'a', 'father'): 0.002906208718626156,\n",
              " ('am', 'a', 'father', 'now'): 0.002937249666221629,\n",
              " ('a', 'father', 'now', '</s>'): 0.002937249666221629,\n",
              " ('father', 'now', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'everything'): 0.0016174863387978142,\n",
              " ('<s>', '<s>', 'everything', 'has'): 0.0028608582574772435,\n",
              " ('<s>', 'everything', 'has', 'changed'): 0.002937249666221629,\n",
              " ('everything', 'has', 'changed', 'but'): 0.002937249666221629,\n",
              " ('has', 'changed', 'but', 'i'): 0.002937249666221629,\n",
              " ('changed', 'but', 'i', 'am'): 0.002937249666221629,\n",
              " ('but', 'i', 'am', 'still'): 0.0029294274300932094,\n",
              " ('i', 'am', 'still', 'the'): 0.0029294274300932094,\n",
              " ('am', 'still', 'the', 'same'): 0.002937249666221629,\n",
              " ('still', 'the', 'same', 'somehow'): 0.0029294274300932094,\n",
              " ('the', 'same', 'somehow', '</s>'): 0.002937249666221629,\n",
              " ('same', 'somehow', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('somehow', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', '<s>', 'you'): 0.05043351548269581,\n",
              " ('<s>', '<s>', 'you', 'know'): 0.0640722724113968,\n",
              " ('<s>', 'you', 'know', 'i'): 0.040762812872467226,\n",
              " ('you', 'know', 'i', 've'): 0.007779171894604768,\n",
              " ('know', 'i', 've', 'never'): 0.0029139072847682124,\n",
              " ('i', 've', 'never', 'been'): 0.0028909329829172143,\n",
              " ('ve', 'never', 'been', 'afraid'): 0.002937249666221629,\n",
              " ('never', 'been', 'afraid', 'of'): 0.002937249666221629,\n",
              " ('been', 'afraid', 'of', 'death'): 0.002937249666221629,\n",
              " ('afraid', 'of', 'death', '</s>'): 0.002937249666221629,\n",
              " ('of', 'death', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('death', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', '<s>', 'but'): 0.039504553734061934,\n",
              " ('<s>', '<s>', 'but', 'now'): 0.00791311093871218,\n",
              " ('<s>', 'but', 'now', 'i'): 0.01083223249669749,\n",
              " ('but', 'now', 'i', 'wanna'): 0.002906208718626156,\n",
              " ('now', 'i', 'wanna', 'see'): 0.002937249666221629,\n",
              " ('i', 'wanna', 'see', 'the'): 0.0029216467463479417,\n",
              " ('wanna', 'see', 'the', 'things'): 0.002937249666221629,\n",
              " ('see', 'the', 'things', 'that'): 0.002937249666221629,\n",
              " ('the', 'things', 'that', 'haven'): 0.002898550724637681,\n",
              " ('things', 'that', 'haven', 't'): 0.002937249666221629,\n",
              " ('that', 'haven', 't', 'happened'): 0.002937249666221629,\n",
              " ('haven', 't', 'happened', 'yet'): 0.002937249666221629,\n",
              " ('t', 'happened', 'yet', '</s>'): 0.002937249666221629,\n",
              " ('happened', 'yet', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('yet', '</s>', '<s>', '<s>'): 0.021231979030144166,\n",
              " ('<s>', '<s>', 'i', 'still'): 0.004933279417711282,\n",
              " ('<s>', 'i', 'still', 'love'): 0.002898550724637681,\n",
              " ('i', 'still', 'love', 'getting'): 0.0029294274300932094,\n",
              " ('still', 'love', 'getting', 'out'): 0.002937249666221629,\n",
              " ('love', 'getting', 'out', 'of'): 0.002937249666221629,\n",
              " ('getting', 'out', 'of', 'my'): 0.002937249666221629,\n",
              " ('out', 'of', 'my', 'mind'): 0.002906208718626156,\n",
              " ('of', 'my', 'mind', 'i'): 0.002937249666221629,\n",
              " ('my', 'mind', 'i', 'should'): 0.00559254327563249,\n",
              " ('mind', 'i', 'should', 'cut'): 0.0029294274300932094,\n",
              " ('i', 'should', 'cut', 'it'): 0.0029294274300932094,\n",
              " ('should', 'cut', 'it', 'down'): 0.002937249666221629,\n",
              " ('cut', 'it', 'down', '</s>'): 0.002937249666221629,\n",
              " ('it', 'down', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('down', '</s>', '<s>', '<s>'): 0.1265497076023392,\n",
              " ('<s>', 'i', 'still', 'know'): 0.002898550724637681,\n",
              " ('i', 'still', 'know', 'people'): 0.002937249666221629,\n",
              " ('still', 'know', 'people', 'i'): 0.002937249666221629,\n",
              " ('know', 'people', 'i', 'don'): 0.002937249666221629,\n",
              " ('people', 'i', 'don', 't'): 0.002937249666221629,\n",
              " ('i', 'don', 't', 'like'): 0.009595484477892756,\n",
              " ('don', 't', 'like', 'and'): 0.002906208718626156,\n",
              " ('t', 'like', 'and', 'i'): 0.002937249666221629,\n",
              " ('like', 'and', 'i', 'should'): 0.002937249666221629,\n",
              " ('and', 'i', 'should', 'cut'): 0.0029294274300932094,\n",
              " ('i', 'should', 'cut', 'them'): 0.0029294274300932094,\n",
              " ('should', 'cut', 'them', 'out'): 0.002937249666221629,\n",
              " ('cut', 'them', 'out', '</s>'): 0.002937249666221629,\n",
              " ('them', 'out', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('out', '</s>', '<s>', '<s>'): 0.11200951248513674,\n",
              " ('<s>', '<s>', 'i', 'feel'): 0.025960372017792156,\n",
              " ('<s>', 'i', 'feel', 'embarrassed'): 0.002712700369913687,\n",
              " ('i', 'feel', 'embarrassed', 'bout'): 0.002937249666221629,\n",
              " ('feel', 'embarrassed', 'bout', 'the'): 0.002937249666221629,\n",
              " ('embarrassed', 'bout', 'the', 'things'): 0.002937249666221629,\n",
              " ('bout', 'the', 'things', 'that'): 0.002937249666221629,\n",
              " ('things', 'that', 'i', 'did'): 0.002906208718626156,\n",
              " ('that', 'i', 'did', 'in'): 0.0029139072847682124,\n",
              " ('i', 'did', 'in', 'my'): 0.002937249666221629,\n",
              " ('did', 'in', 'my', 'youth'): 0.002937249666221629,\n",
              " ('in', 'my', 'youth', '</s>'): 0.002937249666221629,\n",
              " ('my', 'youth', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('youth', '</s>', '<s>', '<s>'): 0.008233731739707836,\n",
              " ('<s>', '<s>', '<s>', 'cause'): 0.014586520947176685,\n",
              " ('<s>', '<s>', 'cause', 'now'): 0.0023231256599788807,\n",
              " ('<s>', 'cause', 'now', 'i'): 0.002937249666221629,\n",
              " ('cause', 'now', 'i', 'have'): 0.002937249666221629,\n",
              " ('now', 'i', 'have', 'a'): 0.002937249666221629,\n",
              " ('i', 'have', 'a', 'child'): 0.0029294274300932094,\n",
              " ('have', 'a', 'child', 'i'): 0.002937249666221629,\n",
              " ('a', 'child', 'i', 'know'): 0.002937249666221629,\n",
              " ('child', 'i', 'know', 'one'): 0.002937249666221629,\n",
              " ('i', 'know', 'one', 'day'): 0.002937249666221629,\n",
              " ('know', 'one', 'day', 'that'): 0.002937249666221629,\n",
              " ('one', 'day', 'that', 'you'): 0.002937249666221629,\n",
              " ('day', 'that', 'you', 'will'): 0.002937249666221629,\n",
              " ('that', 'you', 'will', 'do'): 0.002937249666221629,\n",
              " ('you', 'will', 'do', '</s>'): 0.002937249666221629,\n",
              " ('will', 'do', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'freight'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'freight', 'cargo'): 0.002937249666221629,\n",
              " ('<s>', 'freight', 'cargo', 'dot'): 0.002937249666221629,\n",
              " ('freight', 'cargo', 'dot', 'stops'): 0.002937249666221629,\n",
              " ('cargo', 'dot', 'stops', 'and'): 0.002937249666221629,\n",
              " ('dot', 'stops', 'and', 'aeroplanes'): 0.002937249666221629,\n",
              " ('stops', 'and', 'aeroplanes', '</s>'): 0.002937249666221629,\n",
              " ('and', 'aeroplanes', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('aeroplanes', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'late'): 0.0003060109289617486,\n",
              " ('<s>', '<s>', 'late', 'night'): 0.0029294274300932094,\n",
              " ('<s>', 'late', 'night', 'calls'): 0.002937249666221629,\n",
              " ('late', 'night', 'calls', 'signal'): 0.002937249666221629,\n",
              " ('night', 'calls', 'signal', 'is'): 0.002937249666221629,\n",
              " ('calls', 'signal', 'is', 'in'): 0.002937249666221629,\n",
              " ('signal', 'is', 'in', 'and'): 0.002937249666221629,\n",
              " ('is', 'in', 'and', 'out'): 0.002937249666221629,\n",
              " ('in', 'and', 'out', 'again'): 0.0029294274300932094,\n",
              " ('and', 'out', 'again', '</s>'): 0.002937249666221629,\n",
              " ('out', 'again', '</s>', '<s>'): 0.01086092715231788,\n",
              " ('again', '</s>', '<s>', '<s>'): 0.1580608793686584,\n",
              " ('<s>', '<s>', '<s>', 'feelin'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'feelin', 'low'): 0.002937249666221629,\n",
              " ('<s>', 'feelin', 'low', 'serotonin'): 0.002937249666221629,\n",
              " ('feelin', 'low', 'serotonin', 'known'): 0.002937249666221629,\n",
              " ('low', 'serotonin', 'known', 'better'): 0.002937249666221629,\n",
              " ('serotonin', 'known', 'better', 'days'): 0.002937249666221629,\n",
              " ('known', 'better', 'days', '</s>'): 0.002937249666221629,\n",
              " ('better', 'days', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('days', '</s>', '<s>', '<s>'): 0.01086092715231788,\n",
              " ('<s>', '<s>', '<s>', 'go'): 0.0019089253187613844,\n",
              " ('<s>', '<s>', 'go', 'go'): 0.0054333764553686935,\n",
              " ('<s>', 'go', 'go', 'go'): 0.00559254327563249,\n",
              " ('go', 'go', 'go', 'but'): 0.00559254327563249,\n",
              " ('go', 'go', 'but', 'every'): 0.00559254327563249,\n",
              " ('go', 'but', 'every', 'moment'): 0.00559254327563249,\n",
              " ('but', 'every', 'moment', 'you'): 0.00559254327563249,\n",
              " ('every', 'moment', 'you', 're'): 0.00559254327563249,\n",
              " ('moment', 'you', 're', 'here'): 0.00559254327563249,\n",
              " ('you', 're', 'here', 'with'): 0.00559254327563249,\n",
              " ('re', 'here', 'with', 'me'): 0.00559254327563249,\n",
              " ('here', 'with', 'me', '</s>'): 0.00559254327563249,\n",
              " ('with', 'me', '</s>', '<s>'): 0.0629861982434128,\n",
              " ('me', '</s>', '<s>', '<s>'): 0.4250962278675905,\n",
              " ('<s>', '<s>', '<s>', 'timе'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'timе', 'stops'): 0.002937249666221629,\n",
              " ('<s>', 'timе', 'stops', 'to'): 0.002937249666221629,\n",
              " ('timе', 'stops', 'to', 'still'): 0.002937249666221629,\n",
              " ('stops', 'to', 'still', '</s>'): 0.008233731739707836,\n",
              " ('to', 'still', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('still', '</s>', '<s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', '<s>', 'when'): 0.0144408014571949,\n",
              " ('<s>', '<s>', 'when', 'you'): 0.05735449735449736,\n",
              " ('<s>', 'when', 'you', 'are'): 0.007740324594257179,\n",
              " ('when', 'you', 'are', 'in'): 0.008233731739707836,\n",
              " ('you', 'are', 'in', 'my'): 0.008233731739707836,\n",
              " ('are', 'in', 'my', 'arms'): 0.008233731739707836,\n",
              " ('in', 'my', 'arms', 'it'): 0.008211920529801325,\n",
              " ('my', 'arms', 'it', 'always'): 0.008233731739707836,\n",
              " ('arms', 'it', 'always', 'will'): 0.008233731739707836,\n",
              " ('it', 'always', 'will', '</s>'): 0.008233731739707836,\n",
              " ('always', 'will', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('will', '</s>', '<s>', '<s>'): 0.018659658344283837,\n",
              " ('<s>', '<s>', 'and', 'life'): 0.0028297581013235966,\n",
              " ('<s>', 'and', 'life', 'lifе'): 0.0029216467463479417,\n",
              " ('and', 'life', 'lifе', 'is'): 0.002937249666221629,\n",
              " ('life', 'lifе', 'is', 'changin'): 0.002937249666221629,\n",
              " ('lifе', 'is', 'changin', 'tides'): 0.002937249666221629,\n",
              " ('is', 'changin', 'tides', '</s>'): 0.008233731739707836,\n",
              " ('changin', 'tides', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('tides', '</s>', '<s>', '<s>'): 0.008233731739707836,\n",
              " ('<s>', '<s>', 'i', 'lost'): 0.002507076425394258,\n",
              " ('<s>', 'i', 'lost', 'the'): 0.005577689243027889,\n",
              " ('i', 'lost', 'the', 'confidence'): 0.0029294274300932094,\n",
              " ('lost', 'the', 'confidence', 'in'): 0.002937249666221629,\n",
              " ('the', 'confidence', 'in', 'who'): 0.002937249666221629,\n",
              " ('confidence', 'in', 'who', 'i'): 0.002937249666221629,\n",
              " ('in', 'who', 'i', 'was'): 0.002937249666221629,\n",
              " ('who', 'i', 'was', '</s>'): 0.002937249666221629,\n",
              " ('i', 'was', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('was', '</s>', '<s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', '<s>', 'to'): 0.008320582877959927,\n",
              " ('<s>', '<s>', 'to', 'busy'): 0.0025551684088269454,\n",
              " ('<s>', 'to', 'busy', 'tryna'): 0.002937249666221629,\n",
              " ('to', 'busy', 'tryna', 'chase'): 0.002937249666221629,\n",
              " ('busy', 'tryna', 'chase', 'the'): 0.002937249666221629,\n",
              " ('tryna', 'chase', 'the', 'high'): 0.002937249666221629,\n",
              " ('chase', 'the', 'high', 'and'): 0.002937249666221629,\n",
              " ('the', 'high', 'and', 'get'): 0.002937249666221629,\n",
              " ('high', 'and', 'get', 'the'): 0.002937249666221629,\n",
              " ('and', 'get', 'the', 'numbers'): 0.002937249666221629,\n",
              " ('get', 'the', 'numbers', 'up'): 0.002937249666221629,\n",
              " ('the', 'numbers', 'up', '</s>'): 0.002937249666221629,\n",
              " ('numbers', 'up', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('up', '</s>', '<s>', '<s>'): 0.13664739884393065,\n",
              " ('<s>', 'i', 'have', 'the'): 0.0028608582574772435,\n",
              " ('i', 'have', 'the', 'same'): 0.002937249666221629,\n",
              " ('have', 'the', 'same', 'dream'): 0.002937249666221629,\n",
              " ('the', 'same', 'dream', 'every'): 0.002937249666221629,\n",
              " ('same', 'dream', 'every', 'night'): 0.002937249666221629,\n",
              " ('dream', 'every', 'night', '</s>'): 0.002937249666221629,\n",
              " ('every', 'night', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('night', '</s>', '<s>', '<s>'): 0.09697702539298671,\n",
              " ('<s>', '<s>', '<s>', 'a'): 0.003074681238615665,\n",
              " ('<s>', '<s>', 'a', 'bullet'): 0.002788339670468948,\n",
              " ('<s>', 'a', 'bullet', 'through'): 0.002937249666221629,\n",
              " ('a', 'bullet', 'through', 'my'): 0.002937249666221629,\n",
              " ('bullet', 'through', 'my', 'brain'): 0.002937249666221629,\n",
              " ('through', 'my', 'brain', 'the'): 0.002937249666221629,\n",
              " ('my', 'brain', 'the', 'moment'): 0.002937249666221629,\n",
              " ('brain', 'the', 'moment', 'that'): 0.002937249666221629,\n",
              " ('the', 'moment', 'that', 'i'): 0.00559254327563249,\n",
              " ('moment', 'that', 'i', 'close'): 0.0029216467463479417,\n",
              " ('that', 'i', 'close', 'my'): 0.002937249666221629,\n",
              " ('i', 'close', 'my', 'eyes'): 0.002937249666221629,\n",
              " ('close', 'my', 'eyes', '</s>'): 0.01083223249669749,\n",
              " ('my', 'eyes', '</s>', '<s>'): 0.03138780804150454,\n",
              " ('eyes', '</s>', '<s>', '<s>'): 0.1162130177514793,\n",
              " ('<s>', 'i', 'still', 'have'): 0.002898550724637681,\n",
              " ('i', 'still', 'have', 'to'): 0.002937249666221629,\n",
              " ('still', 'have', 'to', 'lean'): 0.002937249666221629,\n",
              " ('have', 'to', 'lean', 'on'): 0.002937249666221629,\n",
              " ('to', 'lean', 'on', 'a'): 0.002937249666221629,\n",
              " ('lean', 'on', 'a', 'shoulder'): 0.002937249666221629,\n",
              " ('on', 'a', 'shoulder', 'when'): 0.002937249666221629,\n",
              " ('a', 'shoulder', 'when', 'i'): 0.002937249666221629,\n",
              " ('shoulder', 'when', 'i', 've'): 0.002937249666221629,\n",
              " ('when', 'i', 've', 'broken'): 0.0029294274300932094,\n",
              " ('i', 've', 'broken', 'down'): 0.002937249666221629,\n",
              " ('ve', 'broken', 'down', '</s>'): 0.002937249666221629,\n",
              " ('broken', 'down', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', 'and', 'i', 'have'): 0.0018981880931837794,\n",
              " ('and', 'i', 'have', 'people'): 0.002937249666221629,\n",
              " ('i', 'have', 'people', 'that'): 0.002937249666221629,\n",
              " ('have', 'people', 'that', 'depend'): 0.002937249666221629,\n",
              " ('people', 'that', 'depend', 'on'): 0.002937249666221629,\n",
              " ('that', 'depend', 'on', 'me'): 0.002937249666221629,\n",
              " ('depend', 'on', 'me', 'to'): 0.002937249666221629,\n",
              " ('on', 'me', 'to', 'sort'): 0.002937249666221629,\n",
              " ('me', 'to', 'sort', 'them'): 0.002937249666221629,\n",
              " ('to', 'sort', 'them', 'out'): 0.002937249666221629,\n",
              " ('sort', 'them', 'out', '</s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', 'i', 'sometimes'): 0.0008896077638495754,\n",
              " ('<s>', 'i', 'sometimes', 'fantasise'): 0.002937249666221629,\n",
              " ('i', 'sometimes', 'fantasise', 'i'): 0.002937249666221629,\n",
              " ('sometimes', 'fantasise', 'i', 'disappear'): 0.002937249666221629,\n",
              " ('fantasise', 'i', 'disappear', 'without'): 0.002937249666221629,\n",
              " ('i', 'disappear', 'without', 'a'): 0.002937249666221629,\n",
              " ('disappear', 'without', 'a', 'trace'): 0.002937249666221629,\n",
              " ('without', 'a', 'trace', '</s>'): 0.002937249666221629,\n",
              " ('a', 'trace', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('trace', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', 'i', 'have', 'no'): 0.0028608582574772435,\n",
              " ('i', 'have', 'no', 'regrets'): 0.002937249666221629,\n",
              " ('have', 'no', 'regrets', 'but'): 0.002937249666221629,\n",
              " ('no', 'regrets', 'but', 'wish'): 0.002937249666221629,\n",
              " ('regrets', 'but', 'wish', 'i'): 0.002937249666221629,\n",
              " ('but', 'wish', 'i', 'did'): 0.002937249666221629,\n",
              " ('wish', 'i', 'did', 'things'): 0.002937249666221629,\n",
              " ('i', 'did', 'things', 'in'): 0.002937249666221629,\n",
              " ('did', 'things', 'in', 'a'): 0.002937249666221629,\n",
              " ('things', 'in', 'a', 'different'): 0.002937249666221629,\n",
              " ('in', 'a', 'different', 'way'): 0.002937249666221629,\n",
              " ('a', 'different', 'way', '</s>'): 0.002937249666221629,\n",
              " ('different', 'way', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('way', '</s>', '<s>', '<s>'): 0.06533166458072591,\n",
              " ('<s>', '<s>', '<s>', 'low'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'low', 'fly'): 0.002937249666221629,\n",
              " ('<s>', 'low', 'fly', 'zone'): 0.002937249666221629,\n",
              " ('low', 'fly', 'zone', 'lawsuits'): 0.002937249666221629,\n",
              " ('fly', 'zone', 'lawsuits', 'and'): 0.002937249666221629,\n",
              " ('zone', 'lawsuits', 'and', 'film'): 0.002937249666221629,\n",
              " ('lawsuits', 'and', 'film', 'stars'): 0.002937249666221629,\n",
              " ('and', 'film', 'stars', '</s>'): 0.002937249666221629,\n",
              " ('film', 'stars', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('stars', '</s>', '<s>', '<s>'): 0.018659658344283837,\n",
              " ('<s>', '<s>', '<s>', 'headline'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'headline', 'wrote'): 0.002937249666221629,\n",
              " ('<s>', 'headline', 'wrote', 'the'): 0.002937249666221629,\n",
              " ('headline', 'wrote', 'the', 'princess'): 0.002937249666221629,\n",
              " ('wrote', 'the', 'princess', 'and'): 0.002937249666221629,\n",
              " ('the', 'princess', 'and', 'the'): 0.002937249666221629,\n",
              " ('princess', 'and', 'the', 'face'): 0.002937249666221629,\n",
              " ('and', 'the', 'face', 'scar'): 0.002937249666221629,\n",
              " ('the', 'face', 'scar', '</s>'): 0.002937249666221629,\n",
              " ('face', 'scar', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('scar', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'broken'): 0.001034608378870674,\n",
              " ('<s>', '<s>', 'broken', 'bones'): 0.0028909329829172143,\n",
              " ('<s>', 'broken', 'bones', 'break'): 0.002937249666221629,\n",
              " ('broken', 'bones', 'break', 'ins'): 0.002937249666221629,\n",
              " ('bones', 'break', 'ins', 'and'): 0.002937249666221629,\n",
              " ('break', 'ins', 'and', 'babylon'): 0.002937249666221629,\n",
              " ('ins', 'and', 'babylon', '</s>'): 0.002937249666221629,\n",
              " ('and', 'babylon', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('babylon', '</s>', '<s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'time'): 0.0008888888888888888,\n",
              " ('<s>', '<s>', 'time', 'stops'): 0.005533596837944664,\n",
              " ('<s>', 'time', 'stops', 'to'): 0.00559254327563249,\n",
              " ('time', 'stops', 'to', 'still'): 0.00559254327563249,\n",
              " ('<s>', 'and', 'life', 'life'): 0.005577689243027889,\n",
              " ('and', 'life', 'life', 'is'): 0.00559254327563249,\n",
              " ('life', 'life', 'is', 'changin'): 0.00559254327563249,\n",
              " ('life', 'is', 'changin', 'tides'): 0.00559254327563249,\n",
              " ('<s>', '<s>', 'i', 'took'): 0.002507076425394258,\n",
              " ('<s>', 'i', 'took', 'an'): 0.0029216467463479417,\n",
              " ('i', 'took', 'an', 'arrow'): 0.002937249666221629,\n",
              " ('took', 'an', 'arrow', 'to'): 0.002937249666221629,\n",
              " ('an', 'arrow', 'to', 'the'): 0.002937249666221629,\n",
              " ('arrow', 'to', 'the', 'heart'): 0.002937249666221629,\n",
              " ('to', 'the', 'heart', '</s>'): 0.002937249666221629,\n",
              " ('the', 'heart', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('heart', '</s>', '<s>', '<s>'): 0.07688504326328802,\n",
              " ('<s>', '<s>', 'i', 'never'): 0.01544682571775172,\n",
              " ('<s>', 'i', 'never', 'kissed'): 0.0028025477707006373,\n",
              " ('i', 'never', 'kissed', 'a'): 0.002937249666221629,\n",
              " ('never', 'kissed', 'a', 'mouth'): 0.002937249666221629,\n",
              " ('kissed', 'a', 'mouth', 'that'): 0.002937249666221629,\n",
              " ('a', 'mouth', 'that', 'taste'): 0.002937249666221629,\n",
              " ('mouth', 'that', 'taste', 'like'): 0.002937249666221629,\n",
              " ('that', 'taste', 'like', 'yours'): 0.002937249666221629,\n",
              " ('taste', 'like', 'yours', '</s>'): 0.002937249666221629,\n",
              " ('like', 'yours', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('yours', '</s>', '<s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', '<s>', 'strawberries'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'strawberries', 'and'): 0.002937249666221629,\n",
              " ('<s>', 'strawberries', 'and', 'somethin'): 0.002937249666221629,\n",
              " ('strawberries', 'and', 'somethin', 'more'): 0.002937249666221629,\n",
              " ('and', 'somethin', 'more', '</s>'): 0.002937249666221629,\n",
              " ('somethin', 'more', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('more', '</s>', '<s>', '<s>'): 0.03389391979301423,\n",
              " ('<s>', '<s>', 'ooh', 'yeah'): 0.005140758873929009,\n",
              " ('<s>', 'ooh', 'yeah', 'i'): 0.00559254327563249,\n",
              " ('ooh', 'yeah', 'i', 'want'): 0.00559254327563249,\n",
              " ('yeah', 'i', 'want', 'it'): 0.005562913907284768,\n",
              " ('i', 'want', 'it', 'all'): 0.00559254327563249,\n",
              " ('want', 'it', 'all', '</s>'): 0.0029294274300932094,\n",
              " ('it', 'all', '</s>', '<s>'): 0.01086092715231788,\n",
              " ('all', '</s>', '<s>', '<s>'): 0.03138780804150454,\n",
              " ('<s>', '<s>', '<s>', 'lipstick'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'lipstick', 'on'): 0.002937249666221629,\n",
              " ('<s>', 'lipstick', 'on', 'my'): 0.002937249666221629,\n",
              " ('lipstick', 'on', 'my', 'guitar'): 0.002937249666221629,\n",
              " ('on', 'my', 'guitar', 'ooh'): 0.0029294274300932094,\n",
              " ('my', 'guitar', 'ooh', '</s>'): 0.002937249666221629,\n",
              " ('guitar', 'ooh', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'fill'): 0.0003060109289617486,\n",
              " ('<s>', '<s>', 'fill', 'up'): 0.00559254327563249,\n",
              " ('<s>', 'fill', 'up', 'the'): 0.0029294274300932094,\n",
              " ('fill', 'up', 'the', 'engines'): 0.002937249666221629,\n",
              " ('up', 'the', 'engines', 'we'): 0.002937249666221629,\n",
              " ('the', 'engines', 'we', 'can'): 0.002937249666221629,\n",
              " ('engines', 'we', 'can', 'drive'): 0.002937249666221629,\n",
              " ('we', 'can', 'drive', 'real'): 0.002937249666221629,\n",
              " ('can', 'drive', 'real', 'far'): 0.002937249666221629,\n",
              " ('drive', 'real', 'far', '</s>'): 0.002937249666221629,\n",
              " ('real', 'far', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', 'go', 'dancing'): 0.002846054333764554,\n",
              " ('<s>', 'go', 'dancing', 'underneath'): 0.002937249666221629,\n",
              " ('go', 'dancing', 'underneath', 'the'): 0.002937249666221629,\n",
              " ('dancing', 'underneath', 'the', 'stars'): 0.002937249666221629,\n",
              " ('underneath', 'the', 'stars', '</s>'): 0.002937249666221629,\n",
              " ('the', 'stars', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('want', 'it', 'all', 'mm'): 0.0029294274300932094,\n",
              " ('it', 'all', 'mm', '</s>'): 0.002937249666221629,\n",
              " ('all', 'mm', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('mm', '</s>', '<s>', '<s>'): 0.023790849673202615,\n",
              " ('<s>', '<s>', 'ooh', 'you'): 0.007588739290085679,\n",
              " ('<s>', 'ooh', 'you', 'got'): 0.005577689243027889,\n",
              " ('ooh', 'you', 'got', 'me'): 0.00559254327563249,\n",
              " ('you', 'got', 'me', 'feeling'): 0.005533596837944664,\n",
              " ('got', 'me', 'feeling', 'like'): 0.00559254327563249,\n",
              " ('me', 'feeling', 'like', '</s>'): 0.00559254327563249,\n",
              " ('feeling', 'like', '</s>', '<s>'): 0.016073781291172595,\n",
              " ('like', '</s>', '<s>', '<s>'): 0.041335044929396665,\n",
              " ('<s>', '<s>', 'i', 'wanna'): 0.013020622725434696,\n",
              " ('<s>', 'i', 'wanna', 'be'): 0.007958921694480103,\n",
              " ('i', 'wanna', 'be', 'that'): 0.005548216644649934,\n",
              " ('wanna', 'be', 'that', 'guy'): 0.00559254327563249,\n",
              " ('be', 'that', 'guy', '</s>'): 0.00559254327563249,\n",
              " ('that', 'guy', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('guy', '</s>', '<s>', '<s>'): 0.016073781291172595,\n",
              " ('<s>', 'i', 'wanna', 'kiss'): 0.005391527599486521,\n",
              " ('i', 'wanna', 'kiss', 'your'): 0.00559254327563249,\n",
              " ('wanna', 'kiss', 'your', 'eyes'): 0.00559254327563249,\n",
              " ('kiss', 'your', 'eyes', '</s>'): 0.00559254327563249,\n",
              " ('your', 'eyes', '</s>', '<s>'): 0.07459727385377943,\n",
              " ('<s>', 'i', 'wanna', 'drink'): 0.005391527599486521,\n",
              " ('i', 'wanna', 'drink', 'that'): 0.00559254327563249,\n",
              " ('wanna', 'drink', 'that', 'smile'): 0.00559254327563249,\n",
              " ('drink', 'that', 'smile', '</s>'): 0.00559254327563249,\n",
              " ('that', 'smile', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('smile', '</s>', '<s>', '<s>'): 0.02633637548891786,\n",
              " ('<s>', 'i', 'wanna', 'feel'): 0.005391527599486521,\n",
              " ('i', 'wanna', 'feel', 'like'): 0.005562913907284768,\n",
              " ('wanna', 'feel', 'like', 'i'): 0.00559254327563249,\n",
              " ('feel', 'like', 'i', 'm'): 0.01083223249669749,\n",
              " ('like', 'i', 'm', '</s>'): 0.005391527599486521,\n",
              " ('i', 'm', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('m', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', '<s>', 'like'): 0.00409471766848816,\n",
              " ('<s>', '<s>', 'like', 'my'): 0.01021170610211706,\n",
              " ('<s>', 'like', 'my', 'soul'): 0.005562913907284768,\n",
              " ('like', 'my', 'soul', 's'): 0.00559254327563249,\n",
              " ('my', 'soul', 's', 'on'): 0.00559254327563249,\n",
              " ('soul', 's', 'on', 'fire'): 0.00559254327563249,\n",
              " ('s', 'on', 'fire', '</s>'): 0.00559254327563249,\n",
              " ('on', 'fire', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('fire', '</s>', '<s>', '<s>'): 0.046232439335887617,\n",
              " ('<s>', 'i', 'wanna', 'stay'): 0.005391527599486521,\n",
              " ('i', 'wanna', 'stay', 'up'): 0.005562913907284768,\n",
              " ('wanna', 'stay', 'up', 'all'): 0.00559254327563249,\n",
              " ('stay', 'up', 'all', 'day'): 0.005577689243027889,\n",
              " ('up', 'all', 'day', 'and'): 0.00559254327563249,\n",
              " ('all', 'day', 'and', 'all'): 0.00559254327563249,\n",
              " ('day', 'and', 'all', 'night'): 0.00559254327563249,\n",
              " ('and', 'all', 'night', 'mm'): 0.0029294274300932094,\n",
              " ('all', 'night', 'mm', '</s>'): 0.002937249666221629,\n",
              " ('night', 'mm', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', 'yeah', 'you'): 0.02195416164053076,\n",
              " ('<s>', 'yeah', 'you', 'got'): 0.008104575163398693,\n",
              " ('yeah', 'you', 'got', 'me'): 0.008233731739707836,\n",
              " ('you', 'got', 'me', 'singing'): 0.008168642951251647,\n",
              " ('got', 'me', 'singing', 'like'): 0.008211920529801325,\n",
              " ('me', 'singing', 'like', '</s>'): 0.008233731739707836,\n",
              " ('singing', 'like', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', 'ooh', 'i'): 0.0222766217870257,\n",
              " ('<s>', 'ooh', 'i', 'love'): 0.008104575163398693,\n",
              " ('ooh', 'i', 'love', 'it'): 0.018659658344283837,\n",
              " ('i', 'love', 'it', 'when'): 0.018610747051114023,\n",
              " ('love', 'it', 'when', 'you'): 0.018659658344283837,\n",
              " ('it', 'when', 'you', 'do'): 0.018659658344283837,\n",
              " ('when', 'you', 'do', 'it'): 0.018610747051114023,\n",
              " ('you', 'do', 'it', 'like'): 0.018659658344283837,\n",
              " ('do', 'it', 'like', 'that'): 0.018659658344283837,\n",
              " ('it', 'like', 'that', '</s>'): 0.018659658344283837,\n",
              " ('like', 'that', '</s>', '<s>'): 0.02886866059817945,\n",
              " ('that', '</s>', '<s>', '<s>'): 0.06062893081761007,\n",
              " ('<s>', '<s>', 'and', 'when'): 0.023824737562756732,\n",
              " ('<s>', 'and', 'when', 'you'): 0.01777221526908636,\n",
              " ('and', 'when', 'you', 're'): 0.018659658344283837,\n",
              " ('when', 'you', 're', 'close'): 0.017997465145754118,\n",
              " ('you', 're', 'close', 'up'): 0.018659658344283837,\n",
              " ('re', 'close', 'up', 'give'): 0.018659658344283837,\n",
              " ('close', 'up', 'give', 'me'): 0.018659658344283837,\n",
              " ('up', 'give', 'me', 'thе'): 0.0028909329829172143,\n",
              " ('give', 'me', 'thе', 'shivers'): 0.002937249666221629,\n",
              " ('me', 'thе', 'shivers', '</s>'): 0.002937249666221629,\n",
              " ('thе', 'shivers', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('shivers', '</s>', '<s>', '<s>'): 0.018659658344283837,\n",
              " ('<s>', '<s>', '<s>', 'oh'): 0.020561020036429872,\n",
              " ('<s>', '<s>', 'oh', 'baby'): 0.019630709426627792,\n",
              " ('<s>', 'oh', 'baby', 'you'): 0.018513689700130377,\n",
              " ('oh', 'baby', 'you', 'wanna'): 0.018659658344283837,\n",
              " ('baby', 'you', 'wanna', 'dance'): 0.018659658344283837,\n",
              " ('you', 'wanna', 'dance', 'til'): 0.018659658344283837,\n",
              " ('wanna', 'dance', 'til', 'the'): 0.018659658344283837,\n",
              " ('dance', 'til', 'the', 'sunlight'): 0.018659658344283837,\n",
              " ('til', 'the', 'sunlight', 'cracks'): 0.018659658344283837,\n",
              " ('the', 'sunlight', 'cracks', '</s>'): 0.018659658344283837,\n",
              " ('sunlight', 'cracks', '</s>', '<s>'): 0.018659658344283837,\n",
              " ('cracks', '</s>', '<s>', '<s>'): 0.018659658344283837,\n",
              " ('<s>', '<s>', 'and', 'whеn'): 0.0010041077133728891,\n",
              " ('<s>', 'and', 'whеn', 'they'): 0.002937249666221629,\n",
              " ('and', 'whеn', 'they', 'say'): 0.002937249666221629,\n",
              " ('whеn', 'they', 'say', 'the'): 0.002937249666221629,\n",
              " ('they', 'say', 'the', 'party'): 0.018659658344283837,\n",
              " ('say', 'the', 'party', 's'): 0.018659658344283837,\n",
              " ('the', 'party', 's', 'over'): 0.018610747051114023,\n",
              " ('party', 's', 'over', 'then'): 0.018659658344283837,\n",
              " ('s', 'over', 'then', 'we'): 0.018659658344283837,\n",
              " ('over', 'then', 'we', 'bring'): 0.018659658344283837,\n",
              " ('then', 'we', 'bring', 'it'): 0.018659658344283837,\n",
              " ('we', 'bring', 'it', 'right'): 0.018659658344283837,\n",
              " ('bring', 'it', 'right', 'back'): 0.018659658344283837,\n",
              " ('it', 'right', 'back', '</s>'): 0.016031537450722732,\n",
              " ('right', 'back', '</s>', '<s>'): 0.018659658344283837,\n",
              " ('back', '</s>', '<s>', '<s>'): 0.06766541822721599,\n",
              " ('<s>', '<s>', 'and', 'we'): 0.021086261980830672,\n",
              " ('<s>', 'and', 'we', 'say'): 0.010340479192938208,\n",
              " ('and', 'we', 'say', 'ooh'): 0.01086092715231788,\n",
              " ('we', 'say', 'ooh', 'i'): 0.01086092715231788,\n",
              " ('say', 'ooh', 'i', 'love'): 0.01086092715231788,\n",
              " ('up', 'give', 'me', 'the'): 0.016031537450722732,\n",
              " ('give', 'me', 'the', 'shivers'): 0.015947712418300654,\n",
              " ('me', 'the', 'shivers', '</s>'): 0.016073781291172595,\n",
              " ('the', 'shivers', '</s>', '<s>'): 0.016073781291172595,\n",
              " ('<s>', 'and', 'when', 'they'): 0.01777221526908636,\n",
              " ('and', 'when', 'they', 'say'): 0.016031537450722732,\n",
              " ('when', 'they', 'say', 'the'): 0.016073781291172595,\n",
              " ('<s>', '<s>', '<s>', 'into'): 0.001034608378870674,\n",
              " ('<s>', '<s>', 'into', 'the'): 0.010775295663600524,\n",
              " ('<s>', 'into', 'the', 'car'): 0.0029139072847682124,\n",
              " ('into', 'the', 'car', 'on'): 0.002937249666221629,\n",
              " ('the', 'car', 'on', 'the'): 0.002937249666221629,\n",
              " ('car', 'on', 'the', 'back'): 0.002937249666221629,\n",
              " ('on', 'the', 'back', 'seat'): 0.0029139072847682124,\n",
              " ('the', 'back', 'seat', 'in'): 0.0029139072847682124,\n",
              " ('back', 'seat', 'in', 'the'): 0.002937249666221629,\n",
              " ('seat', 'in', 'the', 'moon'): 0.002937249666221629,\n",
              " ('in', 'the', 'moon', 'lit'): 0.002937249666221629,\n",
              " ('the', 'moon', 'lit', 'dark'): 0.002937249666221629,\n",
              " ('moon', 'lit', 'dark', '</s>'): 0.002937249666221629,\n",
              " ('lit', 'dark', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('<s>', '<s>', '<s>', 'wrap'): 0.00016029143897996359,\n",
              " ('<s>', '<s>', 'wrap', 'me'): 0.002937249666221629,\n",
              " ('<s>', 'wrap', 'me', 'up'): 0.002937249666221629,\n",
              " ('wrap', 'me', 'up', 'between'): 0.002937249666221629,\n",
              " ('me', 'up', 'between', 'your'): 0.002937249666221629,\n",
              " ('up', 'between', 'your', 'legs'): 0.002937249666221629,\n",
              " ('between', 'your', 'legs', 'and'): 0.002937249666221629,\n",
              " ('your', 'legs', 'and', 'arms'): 0.002937249666221629,\n",
              " ('legs', 'and', 'arms', '</s>'): 0.002937249666221629,\n",
              " ('and', 'arms', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('arms', '</s>', '<s>', '<s>'): 0.03886743886743887,\n",
              " ('<s>', 'ooh', 'i', 'can'): 0.008104575163398693,\n",
              " ('ooh', 'i', 'can', 't'): 0.008233731739707836,\n",
              " ('i', 'can', 't', 'get'): 0.0051533742331288344,\n",
              " ('can', 't', 'get', 'enough'): 0.00559254327563249,\n",
              " ('t', 'get', 'enough', '</s>'): 0.00559254327563249,\n",
              " ('get', 'enough', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('enough', '</s>', '<s>', '<s>'): 0.016073781291172595,\n",
              " ('<s>', 'you', 'know', 'you'): 0.012157330154946364,\n",
              " ('you', 'know', 'you', 'could'): 0.002868318122555411,\n",
              " ('know', 'you', 'could', 'tear'): 0.0029139072847682124,\n",
              " ('you', 'could', 'tear', 'me'): 0.002937249666221629,\n",
              " ('could', 'tear', 'me', 'apart'): 0.002937249666221629,\n",
              " ('tear', 'me', 'apart', '</s>'): 0.01083223249669749,\n",
              " ('me', 'apart', '</s>', '<s>'): 0.01086092715231788,\n",
              " ('apart', '</s>', '<s>', '<s>'): 0.016073781291172595,\n",
              " ('<s>', '<s>', '<s>', 'put'): 0.0022003642987249543,\n",
              " ('<s>', '<s>', 'put', 'me'): 0.002831402831402832,\n",
              " ('<s>', 'put', 'me', 'back'): 0.002937249666221629,\n",
              " ('put', 'me', 'back', 'together'): 0.002937249666221629,\n",
              " ('me', 'back', 'together', 'and'): 0.002937249666221629,\n",
              " ('back', 'together', 'and', 'take'): 0.002937249666221629,\n",
              " ('together', 'and', 'take', 'my'): 0.002937249666221629,\n",
              " ('and', 'take', 'my', 'heart'): 0.0029294274300932094,\n",
              " ('take', 'my', 'heart', '</s>'): 0.002937249666221629,\n",
              " ('my', 'heart', '</s>', '<s>'): 0.02886866059817945,\n",
              " ('<s>', 'i', 'never', 'thought'): 0.00535031847133758,\n",
              " ('i', 'never', 'thought', 'that'): 0.0029294274300932094,\n",
              " ('never', 'thought', 'that', 'i'): 0.002937249666221629,\n",
              " ('thought', 'that', 'i', 'could'): 0.0029294274300932094,\n",
              " ('that', 'i', 'could', 'love'): 0.002937249666221629,\n",
              " ('i', 'could', 'love', 'this'): 0.002937249666221629,\n",
              " ('could', 'love', 'this', 'hard'): 0.002937249666221629,\n",
              " ('love', 'this', 'hard', '</s>'): 0.002937249666221629,\n",
              " ('this', 'hard', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('hard', '</s>', '<s>', '<s>'): 0.01086092715231788,\n",
              " ('and', 'all', 'night', '</s>'): 0.0029294274300932094,\n",
              " ('all', 'night', '</s>', '<s>'): 0.023790849673202615,\n",
              " ('<s>', '<s>', '<s>', 'baby'): 0.005989071038251366,\n",
              " ('<s>', '<s>', 'baby', 'you'): 0.01712907117008444,\n",
              " ('<s>', 'baby', 'you', 'are'): 0.005519053876478318,\n",
              " ('baby', 'you', 'are', 'so'): 0.00559254327563249,\n",
              " ('you', 'are', 'so', 'hot'): 0.00559254327563249,\n",
              " ('are', 'so', 'hot', 'you'): 0.00559254327563249,\n",
              " ('so', 'hot', 'you', 'make'): 0.00559254327563249,\n",
              " ('hot', 'you', 'make', 'me'): 0.00559254327563249,\n",
              " ('you', 'make', 'me', 'shiver'): 0.010718954248366013,\n",
              " ('make', 'me', 'shiver', '</s>'): 0.008190224570673713,\n",
              " ('me', 'shiver', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('shiver', '</s>', '<s>', '<s>'): 0.008233731739707836,\n",
              " ('<s>', '<s>', '<s>', 'with'): 0.005843351548269581,\n",
              " ('<s>', '<s>', 'with', 'the'): 0.012333736396614267,\n",
              " ('<s>', 'with', 'the', 'fire'): 0.005548216644649934,\n",
              " ('with', 'the', 'fire', 'you'): 0.00559254327563249,\n",
              " ('the', 'fire', 'you', 'got'): 0.00559254327563249,\n",
              " ('fire', 'you', 'got', 'this'): 0.00559254327563249,\n",
              " ('you', 'got', 'this', 'thing'): 0.00559254327563249,\n",
              " ('got', 'this', 'thing', 'is'): 0.00559254327563249,\n",
              " ('this', 'thing', 'is', 'started'): 0.00559254327563249,\n",
              " ('thing', 'is', 'started', '</s>'): 0.00559254327563249,\n",
              " ('is', 'started', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('started', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', 'i', 'don'): 0.06882329154872624,\n",
              " ('<s>', 'i', 'don', 't'): 0.18560523446019628,\n",
              " ('i', 'don', 't', 'want'): 0.03217309501411101,\n",
              " ('don', 't', 'want', 'it'): 0.007838179519595449,\n",
              " ('t', 'want', 'it', 'to'): 0.005577689243027889,\n",
              " ('want', 'it', 'to', 'stop'): 0.00559254327563249,\n",
              " ('it', 'to', 'stop', 'you'): 0.00559254327563249,\n",
              " ('to', 'stop', 'you', 'know'): 0.00559254327563249,\n",
              " ('stop', 'you', 'know', 'you'): 0.00559254327563249,\n",
              " ('you', 'know', 'you', 'make'): 0.005475880052151239,\n",
              " ('know', 'you', 'make', 'me'): 0.00559254327563249,\n",
              " ('make', 'me', 'shiver', 'oh'): 0.002906208718626156,\n",
              " ('me', 'shiver', 'oh', 'oh'): 0.002937249666221629,\n",
              " ('shiver', 'oh', 'oh', '</s>'): 0.002937249666221629,\n",
              " ('oh', 'oh', '</s>', '<s>'): 0.03886743886743887,\n",
              " ('oh', '</s>', '<s>', '<s>'): 0.09037758830694276,\n",
              " ('it', 'right', 'back', 'yeah'): 0.0028909329829172143,\n",
              " ('right', 'back', 'yeah', '</s>'): 0.002937249666221629,\n",
              " ('back', 'yeah', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('yeah', '</s>', '<s>', '<s>'): 0.1978517722878625,\n",
              " ('<s>', '<s>', '<s>', 'if'): 0.01560655737704918,\n",
              " ('<s>', '<s>', 'if', 'i'): 0.03558792924037461,\n",
              " ('<s>', 'if', 'i', 'forget'): 0.005377720870678618,\n",
              " ('if', 'i', 'forget', 'to'): 0.00559254327563249,\n",
              " ('i', 'forget', 'to', 'say'): 0.00559254327563249,\n",
              " ('forget', 'to', 'say', 'goodbye'): 0.00559254327563249,\n",
              " ('to', 'say', 'goodbye', '</s>'): 0.005577689243027889,\n",
              " ('say', 'goodbye', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('goodbye', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', '<s>', 'before'): 0.00263752276867031,\n",
              " ('<s>', '<s>', 'before', 'i'): 0.020689655172413793,\n",
              " ('<s>', 'before', 'i', 'catch'): 0.005504587155963303,\n",
              " ('before', 'i', 'catch', 'the'): 0.00559254327563249,\n",
              " ('i', 'catch', 'the', 'plane'): 0.00559254327563249,\n",
              " ('catch', 'the', 'plane', '</s>'): 0.00559254327563249,\n",
              " ('the', 'plane', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('plane', '</s>', '<s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', '<s>', 'would'): 0.0016174863387978142,\n",
              " ('<s>', '<s>', 'would', 'you'): 0.026267880364109233,\n",
              " ('<s>', 'would', 'you', 'know'): 0.005475880052151239,\n",
              " ('would', 'you', 'know', 'the'): 0.00559254327563249,\n",
              " ('you', 'know', 'the', 'way'): 0.005562913907284768,\n",
              " ('know', 'the', 'way', 'that'): 0.00559254327563249,\n",
              " ('the', 'way', 'that', 'i'): 0.005475880052151239,\n",
              " ('way', 'that', 'i', '</s>'): 0.00559254327563249,\n",
              " ('that', 'i', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('i', '</s>', '<s>', '<s>'): 0.11829988193624558,\n",
              " ('<s>', '<s>', '<s>', 'feel'): 0.0017632058287795993,\n",
              " ('<s>', '<s>', 'feel', 'when'): 0.005447470817120623,\n",
              " ('<s>', 'feel', 'when', 'i'): 0.00559254327563249,\n",
              " ('feel', 'when', 'i', 'm'): 0.00559254327563249,\n",
              " ('when', 'i', 'm', 'away'): 0.007607361963190184,\n",
              " ('i', 'm', 'away', '</s>'): 0.008190224570673713,\n",
              " ('m', 'away', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('away', '</s>', '<s>', '<s>'): 0.11200951248513674,\n",
              " ('<s>', '<s>', 'we', 'll'): 0.038249286393910564,\n",
              " ('<s>', 'we', 'll', 'see'): 0.0027954256670902162,\n",
              " ('we', 'll', 'see', 'the'): 0.002937249666221629,\n",
              " ('ll', 'see', 'the', 'same'): 0.00559254327563249,\n",
              " ('see', 'the', 'same', 'sky'): 0.0029294274300932094,\n",
              " ('the', 'same', 'sky', 'tonight'): 0.002937249666221629,\n",
              " ('same', 'sky', 'tonight', '</s>'): 0.002937249666221629,\n",
              " ('sky', 'tonight', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('tonight', '</s>', '<s>', '<s>'): 0.10348139255702281,\n",
              " ('<s>', '<s>', 'but', 'the'): 0.009464701318851822,\n",
              " ('<s>', 'but', 'the', 'stars'): 0.002898550724637681,\n",
              " ('but', 'the', 'stars', 'are'): 0.002937249666221629,\n",
              " ('the', 'stars', 'are', 'out'): 0.002937249666221629,\n",
              " ('stars', 'are', 'out', 'of'): 0.002937249666221629,\n",
              " ('are', 'out', 'of', 'place'): 0.002937249666221629,\n",
              " ('out', 'of', 'place', '</s>'): 0.002937249666221629,\n",
              " ('of', 'place', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('place', '</s>', '<s>', '<s>'): 0.018659658344283837,\n",
              " ('<s>', '<s>', 'you', 'll'): 0.012647671994440583,\n",
              " ('<s>', 'you', 'll', 'never'): 0.008104575163398693,\n",
              " ('you', 'll', 'never', 'know'): 0.005548216644649934,\n",
              " ('ll', 'never', 'know', 'the'): 0.008211920529801325,\n",
              " ('never', 'know', 'the', 'weight'): 0.005577689243027889,\n",
              " ('know', 'the', 'weight', 'of'): 0.00559254327563249,\n",
              " ('the', 'weight', 'of', 'my'): 0.00559254327563249,\n",
              " ('weight', 'of', 'my', 'heart'): 0.0029294274300932094,\n",
              " ('of', 'my', 'heart', '</s>'): 0.002937249666221629,\n",
              " ('<s>', 'every', 'time', 'i'): 0.0029139072847682124,\n",
              " ('every', 'time', 'i', 'leave'): 0.0029216467463479417,\n",
              " ('time', 'i', 'leave', 'you'): 0.002937249666221629,\n",
              " ('i', 'leave', 'you', 'babe'): 0.002937249666221629,\n",
              " ('leave', 'you', 'babe', '</s>'): 0.002937249666221629,\n",
              " ('you', 'babe', '</s>', '<s>'): 0.002937249666221629,\n",
              " ('babe', '</s>', '<s>', '<s>'): 0.05108005082592122,\n",
              " ('<s>', '<s>', 'it', 's'): 0.1856430707876371,\n",
              " ('<s>', 'it', 's', 'hard'): 0.008788853161843515,\n",
              " ('it', 's', 'hard', 'to'): 0.01080368906455863,\n",
              " ('s', 'hard', 'to', 'break'): 0.005562913907284768,\n",
              " ('hard', 'to', 'break', 'the'): 0.00559254327563249,\n",
              " ('to', 'break', 'the', 'landing'): 0.00559254327563249,\n",
              " ('break', 'the', 'landing', '</s>'): 0.00559254327563249,\n",
              " ('the', 'landing', '</s>', '<s>'): 0.00559254327563249,\n",
              " ('landing', '</s>', '<s>', '<s>'): 0.00559254327563249,\n",
              " ('<s>', '<s>', 'but', 'i'): 0.12893716058960433,\n",
              " ('<s>', 'but', 'i', 'll'): 0.02212486308871851,\n",
              " ('but', 'i', 'll', 'see'): 0.010663198959687906,\n",
              " ('i', 'll', 'see', 'you'): 0.01846553966189857,\n",
              " ('ll', 'see', 'you', 'again'): 0.008147174770039421,\n",
              " ('see', 'you', 'again', '</s>'): 0.008233731739707836,\n",
              " ('you', 'again', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('<s>', '<s>', 'i', 'm'): 0.1286696320258795,\n",
              " ('<s>', 'i', 'm', 'never'): 0.017089201877934272,\n",
              " ('i', 'm', 'never', 'gonna'): 0.03130659767141009,\n",
              " ('m', 'never', 'gonna', 'leave'): 0.023605706874189364,\n",
              " ('never', 'gonna', 'leave', 'your'): 0.023790849673202615,\n",
              " ('gonna', 'leave', 'your', 'life'): 0.023790849673202615,\n",
              " ('leave', 'your', 'life', '</s>'): 0.023790849673202615,\n",
              " ('your', 'life', '</s>', '<s>'): 0.02886866059817945,\n",
              " ('life', '</s>', '<s>', '<s>'): 0.10776583034647551,\n",
              " ('<s>', '<s>', '<s>', 'even'): 0.0016174863387978142,\n",
              " ('<s>', '<s>', 'even', 'at'): 0.008062418725617686,\n",
              " ('<s>', 'even', 'at', 'the'): 0.008233731739707836,\n",
              " ('even', 'at', 'the', 'times'): 0.008233731739707836,\n",
              " ('at', 'the', 'times', 'i'): 0.008233731739707836,\n",
              " ('the', 'times', 'i', 'm'): 0.018610747051114023,\n",
              " ('times', 'i', 'm', 'miles'): 0.008147174770039421,\n",
              " ('i', 'm', 'miles', 'away'): 0.01086092715231788,\n",
              " ('m', 'miles', 'away', '</s>'): 0.008211920529801325,\n",
              " ('miles', 'away', '</s>', '<s>'): 0.01086092715231788,\n",
              " ('<s>', '<s>', 'you', 'are'): 0.023766504517025715,\n",
              " ('<s>', 'you', 'are', 'always'): 0.00793854033290653,\n",
              " ('you', 'are', 'always', 'on'): 0.008233731739707836,\n",
              " ('are', 'always', 'on', 'my'): 0.008233731739707836,\n",
              " ('always', 'on', 'my', 'mind'): 0.008233731739707836,\n",
              " ('on', 'my', 'mind', '</s>'): 0.013438735177865611,\n",
              " ('my', 'mind', '</s>', '<s>'): 0.05348542458808619,\n",
              " ('mind', '</s>', '<s>', '<s>'): 0.08815628815628816,\n",
              " ('<s>', '<s>', '<s>', 'forever'): 0.0004517304189435337,\n",
              " ('<s>', '<s>', 'forever', 'and'): 0.008233731739707836,\n",
              " ('<s>', 'forever', 'and', 'now'): 0.008233731739707836,\n",
              " ('forever', 'and', 'now', 'i'): 0.008233731739707836,\n",
              " ('and', 'now', 'i', 'will'): 0.008062418725617686,\n",
              " ('now', 'i', 'will', 'be'): 0.008233731739707836,\n",
              " ('i', 'will', 'be', 'by'): 0.008211920529801325,\n",
              " ('will', 'be', 'by', 'your'): 0.008233731739707836,\n",
              " ('be', 'by', 'your', 'side'): 0.01086092715231788,\n",
              " ('by', 'your', 'side', '</s>'): 0.013438735177865611,\n",
              " ('your', 'side', '</s>', '<s>'): 0.013474240422721268,\n",
              " ('side', '</s>', '<s>', '<s>'): 0.023790849673202615,\n",
              " ('<s>', '<s>', 'i', 'know'): 0.023534169025475133,\n",
              " ('<s>', 'i', 'know', 'it'): 0.015155279503105589,\n",
              " ('i', 'know', 'it', 'can'): 0.008125819134993446,\n",
              " ('know', 'it', 'can', 'change'): 0.008211920529801325,\n",
              " ('it', 'can', 'change', 'from'): 0.008233731739707836,\n",
              " ('can', 'change', 'from', 'day'): 0.008233731739707836,\n",
              " ('change', 'from', 'day', 'to'): 0.008233731739707836,\n",
              " ('from', 'day', 'to', 'day'): 0.008233731739707836,\n",
              " ('day', 'to', 'day', '</s>'): 0.008211920529801325,\n",
              " ('to', 'day', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('day', '</s>', '<s>', '<s>'): 0.07459727385377943,\n",
              " ('<s>', '<s>', 'but', 'this'): 0.00791311093871218,\n",
              " ('<s>', 'but', 'this', 'love'): 0.008190224570673713,\n",
              " ('but', 'this', 'love', 'will'): 0.008233731739707836,\n",
              " ('this', 'love', 'will', 'keep'): 0.008233731739707836,\n",
              " ('love', 'will', 'keep', 'alive'): 0.008233731739707836,\n",
              " ('will', 'keep', 'alive', '</s>'): 0.008233731739707836,\n",
              " ('keep', 'alive', '</s>', '<s>'): 0.008233731739707836,\n",
              " ('alive', '</s>', '<s>', '<s>'): 0.023790849673202615,\n",
              " ('<s>', '<s>', 'i', 'i'): 0.002507076425394258,\n",
              " ('<s>', 'i', 'i', 'i'): 0.008233731739707836,\n",
              " ('i', 'i', 'i', 'm'): 0.008233731739707836,\n",
              " ('i', 'i', 'm', 'i'): 0.008233731739707836,\n",
              " ('i', 'm', 'i', 'm'): 0.01086092715231788,\n",
              " ('m', 'i', 'm', 'never'): 0.01086092715231788,\n",
              " ('<s>', '<s>', 'oh', 'i'): 0.05267249757045676,\n",
              " ('<s>', 'oh', 'i', 'could'): 0.0052434456928838954,\n",
              " ('oh', 'i', 'could', 'never'): 0.0029294274300932094,\n",
              " ...}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = read_file(\"data/lyrics/ed_sheeran.txt\")\n",
        "train = preprocess(train, n)\n",
        "lm = NGramLanguageModel(n, train, smoothing)\n",
        "lm.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "17794ab7",
      "metadata": {
        "id": "17794ab7"
      },
      "outputs": [],
      "source": [
        "s1 = (\"Every\", \"time\", \"you\",'come')\n",
        "\n",
        "s2 = (\"Fell\", \"the\", \"fall\")\n",
        "\n",
        "s3 = (\"Down Bad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "69ef66a2",
      "metadata": {
        "id": "69ef66a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('around', 0.002937249666221629)]\n"
          ]
        }
      ],
      "source": [
        "print(top_k_best_candidates(lm, s1[-(n-1):], 3, without=['<s>', '</s>']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "560fcf88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Every time you come around </s>', 0.1715183012821963)]\n"
          ]
        }
      ],
      "source": [
        "sentences = list(generate_sentences_from_phrase(lm, 1, list(s1), 1, 'max'))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4e956",
      "metadata": {
        "id": "8ee4e956"
      },
      "source": [
        "### **Written 4.3.2** – Text Generation [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205129a7",
      "metadata": {
        "id": "205129a7"
      },
      "source": [
        "For this subtask, train an RNN LM using `data/ed_sheeran.txt`\n",
        "\n",
        "In this part, we will try the first two approaches to generate sentences.\n",
        "\n",
        "Q1. Use `predict_next_words()` method to generate sentences after the provided phrases from `s1` to `s3`. Use modes `max` and `multinomial`. Report one of your favorite generations (for any strategy or phrase).\n",
        "\n",
        "Q2. Which decoding strategy did you like better and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "7cb85208",
      "metadata": {},
      "outputs": [],
      "source": [
        "s1 = \"yellow\"\n",
        "\n",
        "s2 = \"fell the fall\"\n",
        "\n",
        "s3 = \"down bad\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b9a19423",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(8.524552548567668)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate your RNN model's perplexity\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/ed_sheeran.txt\")\n",
        "glove_embeddings = load_glove_embeddings('data/glove.6B.50d.txt')\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "perplexity = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        #print(output.view(-1, vocab_size).shape)\n",
        "        #print(targets.view(-1).shape)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    perplexity = np.exp(loss.item())\n",
        "\n",
        "perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "dcd3a291",
      "metadata": {
        "id": "dcd3a291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pages </s> <s> i m\n"
          ]
        }
      ],
      "source": [
        "sentence = s1\n",
        "predicted_words_sequence = RNN.generate_sentence(sentence, word_to_ix, ix_to_word, 5, mode='max')\n",
        "print(' '.join(predicted_words_sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b89c7207",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in love with you </s>\n"
          ]
        }
      ],
      "source": [
        "sentence = s2\n",
        "predicted_words_sequence = RNN.generate_sentence(sentence, word_to_ix, ix_to_word, 5, mode='max')\n",
        "print(' '.join(predicted_words_sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c060cc02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fruit </s> <s> i m\n"
          ]
        }
      ],
      "source": [
        "sentence = s3\n",
        "predicted_words_sequence = RNN.generate_sentence(sentence, word_to_ix, ix_to_word, 5, mode='max')\n",
        "print(' '.join(predicted_words_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712619f",
      "metadata": {
        "id": "d712619f"
      },
      "source": [
        "**Aside (for fun!)**: Train your LM on Taylor Swift lyrics and generate the next hit!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b9047d",
      "metadata": {
        "id": "17b9047d"
      },
      "source": [
        "### **Written 4.4** – Battle of the LMs: GPT-2, Trigram and RNN [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d38359",
      "metadata": {
        "id": "41d38359"
      },
      "source": [
        "For this subtask, you will be generating text and comparing GPT-2 with your n-gram and RNN language models. \n",
        "\n",
        "Generative pretrained transformer (GPT) is a neural language model series created by OpenAI. The n-gram language model you trained has on average around 10K-20K parameters (`len(lm.model)`.) Compare that to the 175 billion parameters of GPT-3, which is likely much smaller than more recent iterations (though they don't tell us anymore)!\n",
        "\n",
        "Let's see how GPT-2 compares to the LMs you trained in Written 4.3.1 on the `data/bbc/tech-small.txt` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e143e87c",
      "metadata": {
        "id": "e143e87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "179.0901520291377"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate your n-gram model's perplexity\n",
        "test = preprocess(read_file(\"data/bbc/tech-small.txt\"), 3)\n",
        "NGram = NGramLanguageModel(n=3, train_data=test)\n",
        "NGram.build()\n",
        "NGram.perplexity(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a164e62b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.6010288000106812, Perplexity: 4.958130726323093\n",
            "Epoch 2/10, Loss: 0.4941759705543518, Perplexity: 1.6391469770188427\n",
            "Epoch 3/10, Loss: 0.33784034848213196, Perplexity: 1.4019166674140409\n",
            "Epoch 4/10, Loss: 0.16959521174430847, Perplexity: 1.1848251509322285\n",
            "Epoch 5/10, Loss: 0.2638842463493347, Perplexity: 1.3019774789317493\n",
            "Epoch 6/10, Loss: 0.14845190942287445, Perplexity: 1.1600370095837167\n",
            "Epoch 7/10, Loss: 0.17833548784255981, Perplexity: 1.1952262378516227\n",
            "Epoch 8/10, Loss: 0.16257105767726898, Perplexity: 1.1765319171037545\n",
            "Epoch 9/10, Loss: 0.16952191293239594, Perplexity: 1.1847383078391278\n",
            "Epoch 10/10, Loss: 0.08578262478113174, Perplexity: 1.0895694571618804\n",
            "1.0895694571618804\n"
          ]
        }
      ],
      "source": [
        "# Calculate your RNN model's perplexity\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/bbc/tech-small.txt\")\n",
        "glove_embeddings = load_glove_embeddings('data/glove.6B.50d.txt')\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    perplexity = np.exp(loss.item())\n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db542ceb",
      "metadata": {
        "id": "db542ceb"
      },
      "source": [
        "#### Computing GPT-2's perplexity on the test set\n",
        "\n",
        "You need to enable a GPU runtime from the Colab `Runtime` menu option (you can also use your computer if you have an accelerator). Go to `Runtime` → `Change Runtime Type` → `Hardware Accelerator (GPU)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Ldf6ovg4B5Qc",
      "metadata": {
        "id": "Ldf6ovg4B5Qc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ccy/anaconda3/envs/NLP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "67zwTHSI0hCC",
      "metadata": {
        "id": "67zwTHSI0hCC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "wmQKbMXjDFNj",
      "metadata": {
        "id": "wmQKbMXjDFNj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:05<00:00,  2.37it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 100\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "giXGq0Z0DWdr",
      "metadata": {
        "id": "giXGq0Z0DWdr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity using GPT2: 50.644840240478516\n"
          ]
        }
      ],
      "source": [
        "print(\"Perplexity using GPT2:\", ppl.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
